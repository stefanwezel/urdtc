{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "As we're luckily standing on the shoulders of giants, we can do some imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import imageio\n",
    "import scipy.io as sio\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "Let's load and convert the data, so we can use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = ['cifar10', 'mnist', 'cub', 'awa2',\n",
    "            'imagenetfeatures', 'apyfeatures']\n",
    "# dataset = 'awa2'\n",
    "dataset = 'cub'\n",
    "\n",
    "data_path = '/home/swezel/projects/urdtc/data/'\n",
    "\n",
    "# attribute name lookup (first attr_id is 0) \n",
    "with open (data_path + 'cub/attributes.txt', 'r') as f:\n",
    "    attributes=f.readlines()\n",
    "attribute_name_dict = {str(int(attr.split(' ')[0])-1): attr.split(' ')[1] for attr in attributes}\n",
    "\n",
    "def get_dataset_config(dataset, cnn_type, max_iters):\n",
    "    input_channels = None\n",
    "    if dataset == 'mnist':\n",
    "        input_channels = 1\n",
    "        if cnn_type == 'cnn':\n",
    "            cnn_output_size = 4*4*100\n",
    "        elif cnn_type == 'resnet':\n",
    "            cnn_output_size = 512\n",
    "        elif cnn_type == 'shallowcnn':\n",
    "            cnn_output_size = 4*4*64\n",
    "        out_freq = 100\n",
    "        #assert max_iters > 4\n",
    "    elif dataset == 'cifar10':\n",
    "        input_channels = 3\n",
    "        if cnn_type == 'cnn':\n",
    "            cnn_output_size = 8*8*32\n",
    "        elif cnn_type == 'resnet' or cnn_type == 'resnet18':\n",
    "            cnn_output_size = 512\n",
    "        elif cnn_type == 'shallowcnn':\n",
    "            cnn_output_size = 4*4*64\n",
    "        out_freq = 100\n",
    "        #assert max_iters > 4\n",
    "    elif dataset == 'cub':\n",
    "        input_channels = 3\n",
    "        if cnn_type == 'cnn':\n",
    "            # cnn_output_size = 32*32*32\n",
    "            cnn_output_size = 280900 # the above does not work? Maybe because of dataloader issue?\n",
    "        elif cnn_type == 'resnet' or cnn_type == 'resnet152':\n",
    "            cnn_output_size = 2048\n",
    "        out_freq = 10\n",
    "        #assert max_iters > 8\n",
    "    elif dataset == 'awa2':\n",
    "        input_channels = 3\n",
    "        if cnn_type == 'cnn':\n",
    "#             cnn_output_size = 32*32*32  # TODO: check\n",
    "            cnn_output_size = 2048\n",
    "        elif cnn_type == 'resnet' or cnn_type == 'resnet152':\n",
    "            cnn_output_size = 2048\n",
    "        out_freq = 10\n",
    "        #assert max_iters > 6\n",
    "    elif dataset == 'imagenetfeatures':\n",
    "        cnn_output_size = 2048\n",
    "        out_freq = 100\n",
    "        #assert max_iters > 10\n",
    "    elif dataset == 'apyfeatures':\n",
    "        cnn_output_size = 2048\n",
    "        out_freq = 10\n",
    "        #assert max_iters > 5\n",
    "\n",
    "    return input_channels, cnn_output_size, out_freq\n",
    "\n",
    "class DataLoader(object):\n",
    "    def __init__(self, dataset='mnist'):\n",
    "        assert dataset in DATASETS\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def load_data(self, batch_size=100, num_workers=4, root='./data/'):\n",
    "\n",
    "        if self.dataset == 'mnist':\n",
    "            #transform_train = transforms.ToTensor()\n",
    "            #transform_test = transforms.ToTensor()\n",
    "            class AddGaussianNoise(object):\n",
    "                def __init__(self, mean=0., std=1.):\n",
    "                    self.std = std\n",
    "                    self.mean = mean\n",
    "\n",
    "                def __call__(self, tensor):\n",
    "                    output = tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "                    return output.clamp(0., 1.)\n",
    "\n",
    "                def __repr__(self):\n",
    "                    return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
    "\n",
    "            transform_train = transforms.Compose([\n",
    "               transforms.ToTensor(),\n",
    "               #AddGaussianNoise(0., 0.2)\n",
    "               #transforms.Normalize((0.1307,), (0.3081,))\n",
    "            ])\n",
    "            transform_test = transforms.Compose([\n",
    "               transforms.ToTensor(),\n",
    "               #AddGaussianNoise(0., 0.2)\n",
    "               #transforms.Normalize((0.1307,), (0.3081,))\n",
    "            ])\n",
    "            classes = [i for i in range(10)]\n",
    "            dataset_class = dsets.MNIST\n",
    "\n",
    "        elif self.dataset == 'cifar10':\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "            transform_test = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "            classes = ('plane', 'car', 'bird', 'cat',\n",
    "                       'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "            dataset_class = dsets.CIFAR10\n",
    "\n",
    "        elif self.dataset == 'cub':\n",
    "\n",
    "            transform_train = transforms.Compose([transforms.ToPILImage(),\n",
    "                                                  transforms.RandomResizedCrop(224),\n",
    "                                                  transforms.RandomHorizontalFlip(),\n",
    "                                                  transforms.ToTensor(),\n",
    "                                                  transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                                                                       std=(0.229, 0.224, 0.225))])\n",
    "\n",
    "            transform_test = transforms.Compose([transforms.ToPILImage(),\n",
    "                                                 transforms.Resize(224),\n",
    "                                                 transforms.CenterCrop(224),\n",
    "                                                 transforms.ToTensor(),\n",
    "                                                 transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                                                                      std=(0.229, 0.224, 0.225))])\n",
    "\n",
    "            dataset_class = CUB\n",
    "            classes = list(range(200))\n",
    "\n",
    "        elif self.dataset == 'awa2':\n",
    "            transform_train = transforms.Compose([transforms.ToPILImage(),\n",
    "                                                  transforms.RandomResizedCrop(224),\n",
    "                                                  transforms.RandomHorizontalFlip(),\n",
    "                                                  transforms.ToTensor(),\n",
    "                                                  transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                                                                       std=(0.229, 0.224, 0.225))])\n",
    "\n",
    "            transform_test = transforms.Compose([transforms.ToPILImage(),\n",
    "                                                 transforms.Resize(224),\n",
    "                                                 transforms.CenterCrop(224),\n",
    "                                                 transforms.ToTensor(),\n",
    "                                                 transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                                                                      std=(0.229, 0.224, 0.225))])\n",
    "\n",
    "            dataset_class = AWA2\n",
    "            classes = list(range(50))\n",
    "\n",
    "        elif self.dataset == 'apyfeatures':\n",
    "            transform_train = transforms.ToTensor()\n",
    "            transform_test = transforms.ToTensor()\n",
    "\n",
    "            dataset_class = APYFeatures\n",
    "            classes = list(range(32))\n",
    "\n",
    "        elif self.dataset == 'imagenetfeatures':\n",
    "            transform_train = transforms.ToTensor()\n",
    "            transform_test = transforms.ToTensor()\n",
    "\n",
    "            dataset_class = ImageNetFeatures\n",
    "            classes = list(range(1000))\n",
    "\n",
    "        train_dataset = dataset_class(root=root,\n",
    "                                      train=True,\n",
    "                                      transform=transform_train,\n",
    "                                      download=True)\n",
    "\n",
    "        test_dataset = dataset_class(root=root,\n",
    "                                     train=False,\n",
    "                                     transform=transform_test)\n",
    "\n",
    "        val_size = int(len(train_dataset) * 0.1)\n",
    "        train_size = len(train_dataset) - val_size\n",
    "\n",
    "        train_dataset, val_dataset = torch.utils.data.dataset.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   shuffle=True,\n",
    "                                                   num_workers=num_workers)\n",
    "\n",
    "        val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shuffle=False,\n",
    "                                                 num_workers=num_workers)\n",
    "\n",
    "        test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=False,\n",
    "                                                  num_workers=num_workers)\n",
    "\n",
    "        dataloaders = {'train': train_loader,\n",
    "                       'val': val_loader,\n",
    "                       'test': test_loader}\n",
    "\n",
    "        return dataloaders, classes\n",
    "\n",
    "class CUB(Dataset):\n",
    "    \"\"\"CUB200-2011 dataset.\"\"\"\n",
    "    attribute_file = 'attributes/class_attribute_labels_continuous.txt'\n",
    "\n",
    "    def __init__(self, root, train=True, transform=None, normalize=True,\n",
    "                 download=None):\n",
    "        self.root = os.path.join(root, 'cub')\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.data_dir = os.path.join(self.root, 'images')\n",
    "\n",
    "        train_test_split = pd.read_csv(os.path.join(self.root, 'train_test_split.txt'),\n",
    "                                       sep=' ', index_col=0, header=None)\n",
    "        if train:\n",
    "            is_train_image = 1\n",
    "        else:\n",
    "            is_train_image = 0\n",
    "        self.img_ids = train_test_split[train_test_split[1] == is_train_image].index.tolist()\n",
    "        self.id_to_img = pd.read_csv(os.path.join(self.root, 'images.txt'),\n",
    "                                     sep=' ', index_col=0, header=None)\n",
    "\n",
    "        raw_mtx = np.loadtxt(os.path.join(self.root,\n",
    "                                          self.attribute_file))\n",
    "        raw_mtx[raw_mtx == -1] = 0\n",
    "        raw_mtx = raw_mtx / raw_mtx.max()\n",
    "        self.attribute_mtx = torch.tensor(raw_mtx, dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx]\n",
    "        img_name = self.id_to_img[self.id_to_img.index == img_id].values[0][0]\n",
    "        img_path = os.path.join(self.data_dir, img_name)\n",
    "\n",
    "        img = imageio.imread(img_path, pilmode='RGB')\n",
    "        label = int(img_name[:3]) - 1\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label, img_path\n",
    "\n",
    "class AWA2(Dataset):\n",
    "    \"\"\"Animals with Attributes 2 dataset.\"\"\"\n",
    "    split_file = 'train_test_classification_split.txt'\n",
    "    data_dir = 'awa2'\n",
    "    attribute_file = 'predicate-matrix-continuous.txt'\n",
    "\n",
    "    def __init__(self, root, train=True, transform=None, normalize=True,\n",
    "                 download=None):\n",
    "        self.root = os.path.join(root, self.data_dir)\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "\n",
    "        meta_data = pd.read_csv(os.path.join(self.root,\n",
    "                                             self.split_file),\n",
    "                                sep=' ', index_col=0, header=None)\n",
    "        if train:\n",
    "            is_train_image = 1\n",
    "        else:\n",
    "            is_train_image = 0\n",
    "        self.img_ids = meta_data[meta_data[3] == is_train_image].index.tolist()\n",
    "        self.id_to_img = meta_data\n",
    "\n",
    "        raw_mtx = np.loadtxt(os.path.join(self.root,\n",
    "                                          self.attribute_file))\n",
    "        raw_mtx[raw_mtx == -1] = 0\n",
    "        raw_mtx = raw_mtx / raw_mtx.max()\n",
    "        self.attribute_mtx = torch.tensor(raw_mtx, dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx]\n",
    "        img_meta_data = self.id_to_img[self.id_to_img.index == img_id]\n",
    "        img_name = img_meta_data.values[0][0]\n",
    "        img_path = os.path.join(self.root, img_name)\n",
    "\n",
    "        img = imageio.imread(img_path, pilmode='RGB')\n",
    "        label = img_meta_data.values[0][1] - 1\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "# create dataloader objects for train, val and test\n",
    "dl = DataLoader(dataset=dataset)\n",
    "# dataloaders, classes = dl.load_data(4, 4, data_path)# 128 insted of \n",
    "dataloaders, classes = dl.load_data(64, 4, data_path)# 128 insted of \n",
    "\n",
    "# attributes (312 column vectors with 200 rows) -> each class can be described with 312 attributes\n",
    "# percentage of time, human annotator thought, the attribute was present\n",
    "attribute_mtx = dataloaders['train'].dataset.dataset.attribute_mtx\n",
    "\n",
    "# create binary encoding for class attributes\n",
    "attribute_mtx[attribute_mtx < 0.5] = 0.0\n",
    "attribute_mtx[attribute_mtx >= 0.5] = 1.0\n",
    "attribute_mtx = attribute_mtx.to(device) # cuda\n",
    "attribute_size = attribute_mtx.size(1) # number of available attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Models\n",
    "Here, we continue by defining the various models that our uRDTC consists of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Conv2d(in_channels, 20, kernel_size=3, stride=1),\n",
    "                                 nn.ReLU(True),\n",
    "                                 nn.BatchNorm2d(20),\n",
    "                                 nn.Conv2d(20, 50, kernel_size=5, stride=2),\n",
    "                                 nn.ReLU(True),\n",
    "                                 nn.BatchNorm2d(50),\n",
    "                                 nn.Conv2d(50, 100, kernel_size=5, stride=2),\n",
    "                                 nn.ReLU(True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DropoutCNN(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 20, kernel_size=3, stride=1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, kernel_size=5, stride=2)\n",
    "        self.conv3 = nn.Conv2d(50, 100, kernel_size=5, stride=2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.dropout2d(F.relu(self.conv1(x)), 0.2)\n",
    "        x = F.dropout2d(F.relu(self.conv2(x)), 0.2)\n",
    "        x = F.dropout2d(F.relu(self.conv3(x)), 0.2)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def get_cnn(in_channels, type='cnn', pretrained_cnn_weights=None,\n",
    "            freeze_weights=False, default_pretrained=False):\n",
    "    TYPES = ['cnn', 'dropoutcnn', 'resnet152'] # TYPES = ['cnn', 'shallowcnn', 'resnet', 'resnet152']\n",
    "    assert type in TYPES\n",
    "\n",
    "    if type == 'cnn':\n",
    "        cnn = CNN(in_channels)\n",
    "    if type == 'dropoutcnn':\n",
    "        cnn = DropoutCNN(in_channels)\n",
    "#     if type == 'resnet152':\n",
    "#         cnn = models.resnet152(pretrained=default_pretrained)\n",
    "    else:\n",
    "        cnn = Identity()\n",
    "\n",
    "    # if pretrained_cnn_weights:\n",
    "    #     if type == 'resnet152':\n",
    "    #         cnn.fc = nn.Linear(cnn.fc.in_features, pretrained_cnn_weights['fc.weight'].size(0))\n",
    "    #     cnn.load_state_dict(pretrained_cnn_weights)\n",
    "    if pretrained_cnn_weights:\n",
    "        cnn.load_state_dict(pretrained_cnn_weights)\n",
    "    \n",
    "    return cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OC(nn.Module):\n",
    "    def __init__(self, model_type, num_classes, cnn_type, input_channels, cnn_out_size,\n",
    "                 dataset, decision_size=2, max_iters=20, attribute_size=20, attribute_mtx=None, attribute_coef=0.5, hidden_size=100,\n",
    "                 tau_initial=5, tau_target=0.5, use_pretrained=False, shallow=False, strategy='aRDTC'):\n",
    "        super(OC, self).__init__()\n",
    "        assert model_type in ['xoc'] #, 'ioc']\n",
    "        self.model_type = model_type\n",
    "        self.num_classes = num_classes\n",
    "        self.attribute_size = attribute_size\n",
    "        self.attribute_mtx = attribute_mtx\n",
    "        self.attribute_coef = attribute_coef if attribute_mtx is not None else 0.\n",
    "        self.decision_size = decision_size # change keyword default to 3?\n",
    "        self.tau_initial = tau_initial\n",
    "        self.tau_target = tau_target\n",
    "        self.max_iters = max_iters\n",
    "        self.shallow = shallow\n",
    "        self.stats = defaultdict(list)\n",
    "        self.reduced_vocab_size = 2\n",
    "        self.strategy = strategy\n",
    "\n",
    "        self.no_lstm = False\n",
    "\n",
    "        #self.init_attribute_matrix(attribute_mtx, attribute_size, attribute_coef, use_bin_attr)\n",
    "\n",
    "        self.cnn = self.init_cnn(cnn_type, input_channels, dataset, use_pretrained)\n",
    "        self.init_network(hidden_size, decision_size, num_classes, attribute_size, cnn_out_size, shallow)\n",
    "\n",
    "        self.init_losses()\n",
    "\n",
    "\n",
    "\n",
    "        self.phase = 'train'\n",
    "\n",
    "        # for stats\n",
    "        self.logits_list = []\n",
    "        self.sigmas_list = []\n",
    "        self.binary_features_list = []\n",
    "        self.labels_list = []\n",
    "        self.used_attributes_list = []\n",
    "        self.certain_attrs = []\n",
    "        self.attribute_accuracies = []\n",
    "        self.drop_ratios = []\n",
    "        self.mean_sigmas = []\n",
    "        \n",
    "\n",
    "    def init_network(self, hidden_size, decision_size, num_classes, attribute_size, cnn_out_size, shallow):\n",
    "        assert decision_size > 1\n",
    "\n",
    "        # LSTM initialization parameters\n",
    "        if self.no_lstm:\n",
    "            self.init_h0 = nn.Parameter(torch.zeros(attribute_size * decision_size), requires_grad=False)\n",
    "            self.init_c0 = nn.Parameter(torch.zeros(attribute_size * decision_size), requires_grad=False)\n",
    "        else:\n",
    "            self.init_h0 = nn.Parameter(torch.zeros(hidden_size).uniform_(-0.01, 0.01), requires_grad=True)\n",
    "            self.init_c0 = nn.Parameter(torch.zeros(hidden_size).uniform_(-0.01, 0.01), requires_grad=True)\n",
    "\n",
    "        if self.no_lstm:\n",
    "            self.lstm = lambda x, y: (None, (x.squeeze(), x.squeeze()))\n",
    "        else:\n",
    "            self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "\n",
    "        if self.no_lstm:\n",
    "            classifier_in = attribute_size * decision_size\n",
    "        else:\n",
    "            classifier_in = attribute_size * decision_size\n",
    "\n",
    "        self.classifier = nn.Sequential(#nn.BatchNorm1d(classifier_in) if not self.no_lstm else Identity(),\n",
    "                                        nn.Linear(classifier_in, hidden_size),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                                        nn.BatchNorm1d(hidden_size),\n",
    "                                        nn.Linear(hidden_size, num_classes))\n",
    "\n",
    "        if self.model_type == 'xoc':\n",
    "            if self.no_lstm:\n",
    "                feat_select_in_size = attribute_size * decision_size\n",
    "            else:\n",
    "                feat_select_in_size = hidden_size\n",
    "            feat_select_out_size = attribute_size\n",
    "            pre_lstm_size = attribute_size * decision_size * 2\n",
    "\n",
    "            bin_feat_type = 'shallow' if shallow else 'dropoutmlp' #'mlp'\n",
    "            feat_select_type = 'mlp_small'\n",
    "\n",
    "        elif self.model_type == 'ioc':\n",
    "            bin_feat_type = 'identity'\n",
    "            feat_select_type = 'mlp_big'\n",
    "\n",
    "            if not shallow:\n",
    "                feat_select_in_size = cnn_out_size + hidden_size\n",
    "                feat_select_out_size = decision_size\n",
    "                pre_lstm_size = feat_select_out_size\n",
    "            else:\n",
    "                feat_select_in_size = hidden_size\n",
    "                feat_select_out_size = cnn_out_size * decision_size\n",
    "                pre_lstm_size = decision_size\n",
    "\n",
    "        if feat_select_type == 'mlp_small':\n",
    "            self.feature_selection = nn.Sequential(nn.BatchNorm1d(feat_select_in_size) if not self.no_lstm else Identity(),\n",
    "                                                   nn.Linear(feat_select_in_size , hidden_size),\n",
    "                                                   nn.ReLU(inplace=True),\n",
    "                                                   nn.BatchNorm1d(hidden_size),\n",
    "                                                   nn.Linear(hidden_size, feat_select_out_size))\n",
    "        elif feat_select_type == 'mlp_big':\n",
    "            self.feature_selection = nn.Sequential(nn.BatchNorm1d(feat_select_in_size) if not self.no_lstm else Identity(),\n",
    "                                                   nn.Linear(feat_select_in_size, hidden_size),\n",
    "                                                   nn.ReLU(inplace=True),\n",
    "                                                   nn.BatchNorm1d(hidden_size),\n",
    "                                                   nn.Linear(hidden_size, hidden_size),\n",
    "                                                   nn.ReLU(inplace=True),\n",
    "                                                   nn.BatchNorm1d(hidden_size),\n",
    "                                                   nn.Linear(hidden_size, feat_select_out_size))\n",
    "\n",
    "        if bin_feat_type == 'identity':\n",
    "            self.binary_features = Identity()\n",
    "        elif bin_feat_type == 'shallow':\n",
    "            class AddZeros(nn.Module):\n",
    "                def __init__(self):\n",
    "                    super().__init__()\n",
    "\n",
    "                def forward(self, x):\n",
    "                    zeros = torch.zeros_like(x).unsqueeze(2)\n",
    "                    return torch.cat((x.unsqueeze(2), zeros), dim=2)\n",
    "\n",
    "            self.binary_features = AddZeros()\n",
    "        elif bin_feat_type == 'mlp':\n",
    "            self.binary_features = nn.Sequential(nn.BatchNorm1d(cnn_out_size), # use dropout\n",
    "                                                 nn.Linear(cnn_out_size, hidden_size),\n",
    "                                                 nn.ReLU(inplace=True),\n",
    "                                                 nn.BatchNorm1d(hidden_size), # use dropout\n",
    "                                                 nn.Linear(hidden_size, hidden_size),\n",
    "                                                 nn.ReLU(inplace=True),\n",
    "                                                 nn.BatchNorm1d(hidden_size), # use dropout\n",
    "                                                 nn.Linear(hidden_size, attribute_size * self.reduced_vocab_size))\n",
    "        elif bin_feat_type == 'dropoutmlp':\n",
    "            self.binary_features = nn.Sequential(\n",
    "                                                nn.Linear(cnn_out_size, hidden_size),\n",
    "                                                nn.ReLU(inplace=False),\n",
    "                                                nn.Dropout(0.2, inplace=False),\n",
    "                                                nn.Linear(hidden_size, hidden_size),\n",
    "                                                nn.ReLU(inplace=False),\n",
    "                                                nn.Dropout(0.2, inplace=False),\n",
    "                                                nn.Linear(hidden_size, attribute_size * self.reduced_vocab_size)\n",
    "                                                )\n",
    "\n",
    "        if self.no_lstm:\n",
    "            self.pre_lstm = Identity()\n",
    "        else:\n",
    "            self.pre_lstm = nn.Sequential(#nn.BatchNorm1d(pre_lstm_size),\n",
    "                                          nn.Linear(pre_lstm_size, hidden_size),\n",
    "                                          nn.ReLU(inplace=True),\n",
    "                                          nn.BatchNorm1d(hidden_size))\n",
    "\n",
    "\n",
    "        # Temperature parameters\n",
    "        self.binary_features.tau = nn.Parameter(torch.tensor([self.tau_initial], dtype=torch.float), requires_grad=True)\n",
    "        self.feature_selection.tau = nn.Parameter(torch.tensor([self.tau_initial], dtype=torch.float), requires_grad=True)\n",
    "        #self.init_weights()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def get_attribute_uncertainty_batch(self, image_features, n=100, batch_size=64):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            outputs = torch.zeros((n, batch_size, attribute_size * self.reduced_vocab_size), device=device) # init outputs with dummy values\n",
    "\n",
    "    #         if self.phase=='test':\n",
    "    #         print(self.binary_features.training)\n",
    "\n",
    "            if not self.binary_features.training:\n",
    "                current_phase = 'test'\n",
    "                self.binary_features.train()\n",
    "\n",
    "            else:\n",
    "                current_phase = 'train'\n",
    "    #         print(self.binary_features.training)\n",
    "    #         print()\n",
    "\n",
    "    #             for layer in self.binary_features:\n",
    "    #                 if isinstance(layer, torch.nn.modules.dropout._DropoutNd):\n",
    "    #                     layer.train()\n",
    "\n",
    "            for i in range(n):\n",
    "                outputs[i] = F.softmax(self.binary_features(image_features))\n",
    "\n",
    "            if current_phase=='test':\n",
    "    #         if self.phase=='test':\n",
    "                self.binary_features.eval()\n",
    "    #             for layer in self.binary_features:\n",
    "    #                 if isinstance(layer, torch.nn.modules.dropout._DropoutNd):\n",
    "    #                     layer.eval()\n",
    "    #                 else:\n",
    "    #                     layer.train()\n",
    "\n",
    "            sigmas = outputs.std(dim=0)\n",
    "\n",
    "            return sigmas\n",
    "    \n",
    "    \n",
    "    def get_attribute_uncertainty(self, image_features, n=10, batch_size=64):\n",
    "        outputs = torch.zeros((n, batch_size, attribute_size * self.reduced_vocab_size), device=device)\n",
    "        \n",
    "        if self.phase == 'train':\n",
    "            for i in range(n):\n",
    "                outputs[i] = F.softmax(self.binary_features(image_features))\n",
    "        if self.phase == 'test':\n",
    "            for layer in self.binary_features:\n",
    "                if isinstance(layer, torch.nn.modules.dropout._DropoutNd):\n",
    "                    layer.train()\n",
    "#             self.binary_features.train()\n",
    "            for i in range(n):\n",
    "                outputs[i] = F.softmax(self.binary_features(image_features))\n",
    "\n",
    "            for layer in self.binary_features:\n",
    "                if isinstance(layer, torch.nn.modules.dropout._DropoutNd):\n",
    "                    layer.eval()\n",
    "#             self.binary_features.eval()\n",
    "        \n",
    "        sigmas = outputs.std(dim=0)\n",
    "#         print(sigmas.mean())\n",
    "\n",
    "        return sigmas\n",
    "  \n",
    "    def init_attribute_matrix(self, attribute_mtx, attribute_size, attribute_coef, use_bin_attr):\n",
    "        if attribute_coef > 0.:\n",
    "            if use_bin_attr:\n",
    "                attribute_mtx[attribute_mtx < 0.5] = 0.\n",
    "                attribute_mtx[attribute_mtx >= 0.5] = 1.\n",
    "            self.attribute_mtx = nn.Parameter(attribute_mtx, requires_grad=False)\n",
    "            self.attribute_size = attribute_mtx.size(1)\n",
    "        else:\n",
    "            self.attribute_mtx = None\n",
    "            self.attribute_size = attribute_size\n",
    "\n",
    "    def toggle_update_schedule(self):\n",
    "        # TODO: see a few lines below\n",
    "        #self.update_binary_features = not self.update_binary_features\n",
    "        pass\n",
    "\n",
    "    def get_param_groups(self):\n",
    "        cnn_params = []\n",
    "        tree_params = []\n",
    "        for n, p in self.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                # TODO: introduce parameter that allows to switch between training alternatingly\n",
    "                # Currently commented out, so both groups contain the same parameters\n",
    "                \"\"\"\n",
    "                if n.startswith('cnn') or n.startswith('binary_features'):\n",
    "                    print('CNN', n)\n",
    "                    cnn_params.append(p)\n",
    "                else:\n",
    "                    print('OTHER', n)\n",
    "                    tree_params.append(p)\n",
    "                \"\"\"\n",
    "                cnn_params.append(p)\n",
    "                tree_params.append(p)\n",
    "        return tree_params, cnn_params\n",
    "\n",
    "    def set_optimizer(self, optimizers):\n",
    "        self.tree_optimizer = optimizers[0]\n",
    "        self.cnn_optimizer = optimizers[1]\n",
    "\n",
    "    def set_scheduler(self, schedulers):\n",
    "        self.tree_scheduler = schedulers[0]\n",
    "        self.cnn_scheduler = schedulers[1]\n",
    "\n",
    "    def get_optimizer(self):\n",
    "        if self.update_binary_features:\n",
    "            return self.cnn_optimizer\n",
    "        else:\n",
    "            return self.tree_optimizer\n",
    "\n",
    "    def get_scheduler(self):\n",
    "        if self.update_binary_features:\n",
    "            return self.cnn_scheduler\n",
    "        else:\n",
    "            return self.tree_scheduler\n",
    "\n",
    "    def init_losses(self):\n",
    "        self.cls_loss = nn.CrossEntropyLoss()\n",
    "        self.attr_loss = nn.BCEWithLogitsLoss()\n",
    "        self.update_binary_features = False\n",
    "\n",
    "    def init_cnn(self, cnn_type, input_channels, dataset, use_pretrained):\n",
    "        if cnn_type == 'None':\n",
    "            cnn = Identity()\n",
    "        else:\n",
    "            if use_pretrained:\n",
    "                # TODO add data_path and change state dict name \n",
    "                # cnn_state_dict = torch.load('pretrained/{}_{}.pth'.format(dataset, cnn_type))\n",
    "#                 cnn_state_dict = torch.load('pretrained/cub_resnet152.pkl')# .format(dataset, cnn_type)\n",
    "                cnn_state_dict = torch.load('pretrained/{}_resnet152.pkl'.format(dataset))# .format(dataset, cnn_type)\n",
    "\n",
    "                cnn = get_cnn(input_channels, cnn_type, cnn_state_dict, freeze_weights=True)\n",
    "            else:\n",
    "                cnn = get_cnn(input_channels, cnn_type)\n",
    "\n",
    "        return cnn\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.1)\n",
    "\n",
    "    def set_tau(self, epoch):\n",
    "        annealing_factor = epoch / 100\n",
    "        self.tau = self.tau_initial\n",
    "        self.tau -= (self.tau_initial - self.tau_target) * annealing_factor\n",
    "        self.tau = max(self.tau, self.tau_target)\n",
    "\n",
    "    def process_images(self, images):\n",
    "        batch_size = images.size(0)\n",
    "        img_feats = self.cnn(images)\n",
    "        img_feats = img_feats.view(img_feats.size(0), -1)\n",
    "        image_features = self.binary_features(img_feats)\n",
    "        # print(image_features.size())\n",
    "        ################## remove attrs code\n",
    "#         sigmas = self.get_attribute_uncertainty_batch(img_feats,n=100, batch_size=batch_size)\n",
    "        \n",
    "        if self.strategy == 'remRDTC':\n",
    "            with torch.no_grad():\n",
    "                sigmas = self.get_attribute_uncertainty(img_feats, n=5, batch_size=images.size(0))\n",
    "            uncertain_attrs = (sigmas > 0.005).float() # get binary uncertain attrs\n",
    "            certain_attrs = 1. - uncertain_attrs\n",
    "            mask = certain_attrs.detach()\n",
    "            inv_mask = 1-mask\n",
    "            min_value = image_features.min()\n",
    "            image_features = image_features * mask # put zeros where uncertain attts are\n",
    "            image_features = image_features - (inv_mask.detach()*min_value.detach()) #*-500\n",
    "\n",
    "#         sigmas.detach()\n",
    "#         sigmas.cuda()\n",
    "#         self.mean_sigmas.append(sigmas.mean().item())\n",
    "#         drop_ratio = (sigmas>0.005).float().sum()/(images.size(0)*attribute_size*2.)\n",
    "#         min_value = image_features.min()\n",
    "#         min_value.detach()\n",
    "#         self.drop_ratios.append(drop_ratio)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #############################################################\n",
    "        # if not self.training:\n",
    "\n",
    "        #     self.sigmas_list.append(sigmas)\n",
    "            # self.logits_list.append(image_features)\n",
    "            # image_features = torch.cat((attribute_logits, sigmas),1)\n",
    "        \n",
    "        \n",
    "        ################## remove attrs\n",
    "#         uncertain_attrs = (sigmas > 0.005).float() # get binary uncertain attrs\n",
    "#         certain_attrs = 1. - uncertain_attrs\n",
    "#         mask = certain_attrs.detach()#(torch.FloatTensor(image_features.size()).uniform_() > 0.050).float().to(device)\n",
    "        if self.strategy == 'randRDTC':\n",
    "            mask = (torch.FloatTensor(image_features.size()).uniform_() > 0.050).float().to(device)\n",
    "            inv_mask = 1-mask\n",
    "#         self.drop_ratios.append((inv_mask.sum()/39936.0).item())\n",
    "#         mask.detach()\n",
    "            min_value = image_features.min()\n",
    "            image_features = image_features * mask # put zeros where uncertain attts are\n",
    "            image_features = image_features - (inv_mask.detach()*min_value.detach()) #*-500\n",
    "        ##############################################################\n",
    "\n",
    "        if self.model_type == 'xoc':\n",
    "            attribute_logits = image_features.view(-1, 2)\n",
    "\n",
    "\n",
    "            attributes_softmax = F.softmax(attribute_logits / self.binary_features.tau, dim=1)\n",
    "            attributes_hard = self.argmax(attributes_softmax, dim=1)\n",
    "            image_features = attributes_hard.view(images.size(0), -1, 2)\n",
    "\n",
    "            # TODO: generalize to different decision sizes\n",
    "            bin_attribute_logits = attribute_logits - attribute_logits[:, 1].unsqueeze(-1)\n",
    "            self.attribute_logits = bin_attribute_logits[:, 0].view(images.size(0), -1)\n",
    "\n",
    "            self.collect_hist_stats('AttributesSoft', F.softmax(attribute_logits, dim=1))\n",
    "            self.collect_hist_stats('AttributesSoftTemp', attributes_softmax)\n",
    "            self.collect_hist_stats('AttributesHard', attributes_hard.max(dim=1)[1])\n",
    "\n",
    "        return image_features\n",
    "\n",
    "    def make_decision(self, lstm_out, binary_features, iter, sigma=[], max_uncertainty=0.2):\n",
    "        if self.model_type == 'xoc':\n",
    "            # Perform categorical feature selection\n",
    "            selection_logits = self.feature_selection(lstm_out)\n",
    "            # print(selection_logits[0][:10])\n",
    "            if self.training:\n",
    "                hard_selection = F.gumbel_softmax(selection_logits, tau=self.feature_selection.tau, hard=True)\n",
    "            else:\n",
    "               \n",
    "                # sigma = self.get_attribute_uncertainty(binary_features)\n",
    "                # sigma = sigma[:,:312]#.squeeze() \n",
    "                # certain_logits = torch.where(sigma>max_uncertainty,\n",
    "                #                              selection_logits,\n",
    "                #                              torch.zeros_like(selection_logits).new_full(selection_logits.size(), -10000)) # -10000\n",
    "                hard_selection = self.argmax(selection_logits, dim=1)\n",
    "                # print(hard_selection.size())\n",
    "                # print(binary_features.size())\n",
    "                # print(hard_selection.unsqueeze(2).size())\n",
    "\n",
    "\n",
    "            # Get single decision\n",
    "            self.saved_attribute_selection = hard_selection.max(dim=1)[1]\n",
    "            \n",
    "\n",
    "            decision = (hard_selection.unsqueeze(2) * binary_features).view(-1, self.attribute_size * self.decision_size)\n",
    "            # print(decision[0])\n",
    "        elif self.model_type == 'ioc':\n",
    "            if not self.shallow:\n",
    "                features = torch.cat((lstm_out, binary_features), dim=1)\n",
    "                selection_logits = self.feature_selection(features)\n",
    "            else:\n",
    "                shallow_weights = self.feature_selection(lstm_out)\n",
    "                shallow_weights = shallow_weights.view(lstm_out.size(0), -1, self.decision_size)\n",
    "                selection_logits = torch.bmm(binary_features.unsqueeze(1), shallow_weights)\n",
    "                selection_logits = selection_logits.squeeze()\n",
    "\n",
    "            soft_decision = F.softmax(selection_logits / self.tau_selection, dim=1)\n",
    "            hard_selection = self.argmax(soft_decision, dim=1)\n",
    "            decision = hard_selection\n",
    "\n",
    "        # Collect statistics\n",
    "        self.collect_hist_stats('SelectionSoft', F.softmax(selection_logits, dim=1).max(dim=1)[0], iter)\n",
    "        self.collect_hist_stats('SelectionSoftTemp', F.softmax(selection_logits / self.feature_selection.tau, dim=1).max(dim=1)[0], iter)\n",
    "        self.collect_hist_stats('SelectionHard', hard_selection.max(dim=1)[1], iter)\n",
    "\n",
    "        return decision\n",
    "\n",
    "    def get_initial_state(self, batch_size):\n",
    "        h0 = self.init_h0.view(1, 1, -1).expand(-1, batch_size, -1)\n",
    "        c0 = self.init_c0.view(1, 1, -1).expand(-1, batch_size, -1)\n",
    "        state = (h0.contiguous(), c0.contiguous())\n",
    "        return state\n",
    "\n",
    "    def argmax(self, y_soft, dim):\n",
    "        index = y_soft.max(dim, keepdim=True)[1]\n",
    "        y_hard = torch.zeros_like(y_soft).scatter_(dim, index, 1.0)\n",
    "        argmax = y_hard - y_soft.detach() + y_soft\n",
    "        return argmax\n",
    "\n",
    "    def collect_hist_stats(self, name, data, i=None):\n",
    "        # TODO: investigate performance impact of collecting these statistics,\n",
    "        # make option to disable and/or autodisable if tree is too large\n",
    "        if 'Hard' in name:\n",
    "            stat_str = 'Hist/' + name\n",
    "            data = data.detach().cpu()\n",
    "            self.stats[stat_str].append(data)\n",
    "            if i is not None:\n",
    "                stat_str += str(i)\n",
    "                self.stats[stat_str].append(data)\n",
    "\n",
    "    def get_hist_stats(self, reset=True):\n",
    "        stats = self.stats\n",
    "        if reset:\n",
    "            self.stats = defaultdict(list)\n",
    "        #return None\n",
    "        return stats\n",
    "\n",
    "    def reset_stats(self):\n",
    "        self.unique_attributes = [set() for i in range(self.max_iters)]\n",
    "        if self.attribute_coef > 0.:\n",
    "            self.attr_pred_correct = [0 for i in range(self.max_iters)]\n",
    "\n",
    "    def update_unique_attributes(self, unique_attributes, iter):\n",
    "        for attr in unique_attributes:\n",
    "            self.unique_attributes[iter].add(attr.item())\n",
    "\n",
    "    def get_unique_attributes(self):\n",
    "        uniq_per_iter = []\n",
    "        for i in range(self.max_iters):\n",
    "            iter_set = self.unique_attributes[i]\n",
    "            for j in range(i+1):\n",
    "                if j == i:\n",
    "                    continue\n",
    "                iter_set = iter_set.union(self.unique_attributes[j])\n",
    "            uniq_per_iter.append(len(iter_set))\n",
    "        return uniq_per_iter\n",
    "\n",
    "    def update_attr_preds(self, attr_correct, iter):\n",
    "        self.attr_pred_correct[iter] += attr_correct\n",
    "\n",
    "    def get_attr_acc(self, total_cnt):\n",
    "        correct_cumsum = np.cumsum(self.attr_pred_correct)\n",
    "        cnt_per_iter = (np.arange(self.max_iters) + 1) * total_cnt\n",
    "        return correct_cumsum / cnt_per_iter\n",
    "\n",
    "    def init_tree_stats(self):\n",
    "        # TODO: investigate performance impact of collecting these statistics,\n",
    "        # make option to disable and/or autodisable if tree is too large\n",
    "\n",
    "        # Would be nice if this worked with sparse tensors\n",
    "        n_possible_states = self.decision_size ** self.max_iters\n",
    "        self.label_stats = torch.zeros((n_possible_states * self.decision_size,\n",
    "                                        self.num_classes), dtype=torch.int32)\n",
    "                                       #layout=torch.sparse_coo)\n",
    "        self.selection_stats = torch.zeros((n_possible_states,\n",
    "                                            self.attribute_size),\n",
    "                                           dtype=torch.int32)\n",
    "                                           #layout=torch.sparse_coo)\n",
    "\n",
    "    def update_tree_stats(self, attribute_selection, attribute_decisions, labels, iter):\n",
    "        # TODO: investigate performance impact of collecting these statistics,\n",
    "        # make option to disable and/or autodisable if tree is too large\n",
    "\n",
    "        if iter == 0:\n",
    "            self.batch_states = torch.zeros_like(labels)\n",
    "            for i in range(labels.size(0)):\n",
    "                self.label_stats[self.batch_states[i], labels[i]] += 1\n",
    "\n",
    "        for i in range(labels.size(0)):\n",
    "            self.selection_stats[self.batch_states[i], attribute_selection[i]] += 1\n",
    "            self.batch_states[i] += (attribute_decisions[i] + 1) * self.decision_size ** iter\n",
    "            self.label_stats[self.batch_states[i], labels[i]] += 1\n",
    "\n",
    "    def run_iteration(self, binary_features, state, decision_hist, iter, sigma=[]): # also pass sigma here\n",
    "        lstm_out = state[0].squeeze(0)\n",
    "\n",
    "        # Make binary decision\n",
    "        decision = self.make_decision(lstm_out, binary_features, iter, sigma)\n",
    "\n",
    "        if decision_hist is None:\n",
    "            decision_hist = decision\n",
    "        else:\n",
    "            decision_hist = (decision_hist + decision).clamp(0., 1.)\n",
    "\n",
    "        scaled_dh = decision_hist / decision_hist.sum(dim=1).unsqueeze(1).detach()\n",
    "        if self.no_lstm:\n",
    "            lstm_in = scaled_dh\n",
    "        else:\n",
    "            lstm_in = torch.cat((scaled_dh, decision), dim=1)\n",
    "\n",
    "        # Update LSTM state\n",
    "        lstm_in = self.pre_lstm(lstm_in).unsqueeze(1)\n",
    "        _, state = self.lstm(lstm_in, state)\n",
    "\n",
    "        # Get current classification\n",
    "        classifier_in = scaled_dh\n",
    "        #classifier_in = state[1].squeeze(0)\n",
    "        #classifier_in = torch.cat((decision_hist, self.lstm_state_bn(lstm_state)), dim=1)\n",
    "        classification = self.classifier(classifier_in)\n",
    "\n",
    "        return classification, state, decision_hist\n",
    "\n",
    "    def tree_rollout(self, images, labels, keep_tree_stats=False):\n",
    "        # Set initial state\n",
    "        state = self.get_initial_state(images.size(0))\n",
    "\n",
    "        # Get categorical features once\n",
    "        binary_features = self.process_images(images)\n",
    "        # collect attribute stats\n",
    "        # self.binary_features_list.append(binary_features)\n",
    "        # self.labels_list.append(labels)\n",
    "\n",
    "#         attr_acc = (binary_features[:,:,0] == attribute_mtx[labels]).sum().long() / 19968.0 #/ (312*labels.size(0))\n",
    "        attr_acc = (binary_features[:,:,0] == attribute_mtx[labels]).sum().long() / float((attribute_size*labels.size(0)))    \n",
    "#         print(attr_acc)\n",
    "        self.attribute_accuracies.append(attr_acc.item())\n",
    "\n",
    "        ######################### extended vocab code \n",
    "        if self.strategy == 'extRDTC':\n",
    "            with torch.no_grad():\n",
    "                img_feats = self.cnn(images)\n",
    "                img_feats = img_feats.view(img_feats.size(0), -1)\n",
    "    #             image_features = self.binary_features(img_feats) # maybe do those with no grad as well\n",
    "                sigmas = self.get_attribute_uncertainty(img_feats, n=5, batch_size=images.size(0))\n",
    "\n",
    "    #         sigmas.detach()\n",
    "        \n",
    "#         ########### remove attrs code\n",
    "#         uncertain_attrs = (sigmas > 0.005).float()\n",
    "#         certain_attrs = 1. - uncertain_attrs\n",
    "#         uncertain_attrs = uncertain_attrs.view(images.size(0), attribute_size, 2)\n",
    "#         certain_attrs = certain_attrs.view(images.size(0), attribute_size, 2)\n",
    "# #         print(certain_attrs[0])\n",
    "#         mask = torch.bernoulli(torch.empty(binary_features.size(), device=device).uniform_(1, 1))\n",
    "        \n",
    "#         binary_features = binary_features * mask#certain_attrs.detach() # clean binary_features from uncertain attrs (replace with 0)\n",
    "        \n",
    "        \n",
    "\n",
    "#         ######################################### remove attrs code end\n",
    "        \n",
    "# #         self.mean_sigmas.append(sigmas.mean().item())\n",
    "            sigmas = (sigmas > 0.005).float() # 0.004\n",
    "            sigmas_new = torch.zeros(images.size(0), attribute_size,1, device=device)\n",
    "            sigmas_new += sigmas.view(images.size(0),attribute_size,2)[:,:,0].unsqueeze(2)\n",
    "            sigmas_new += sigmas.view(images.size(0),attribute_size,2)[:,:,1].unsqueeze(2)\n",
    "            sigmas_new = (sigmas_new > 0.0).float() # <- denotes uncertain attributes\n",
    "\n",
    "            self.drop_ratios.append((sigmas_new.sum()/19968.0).item())\n",
    "\n",
    "            # obtain uncertainty and append to decision\n",
    "            new_attribute_decisions = torch.cat([binary_features, torch.zeros_like(sigmas_new, device=device)], dim=2)\n",
    "            certain_decisions = 1. - sigmas_new\n",
    "            uncertain_onehot = torch.tensor([[0., 0., 1.]], device=device).repeat(images.size(0), attribute_size, 1)\n",
    "            uncertain_attrs_removed = certain_decisions.detach() * new_attribute_decisions\n",
    "            shaped_uncertain_attrs = sigmas_new.detach() * uncertain_onehot\n",
    "            final_attribute_decisions = uncertain_attrs_removed + shaped_uncertain_attrs\n",
    "            binary_features = final_attribute_decisions\n",
    "#         ######################### extended vocab code end\n",
    "        \n",
    "        \n",
    "\n",
    "        loss = 0\n",
    "        j = 0\n",
    "        # stats\n",
    "        all_classifications = []\n",
    "        all_chosen_attr = []\n",
    "        all_attribute_preds = []\n",
    "\n",
    "        decision_hist = None\n",
    "        while j < self.max_iters:\n",
    "            classification, state, decision_hist = self.run_iteration(binary_features, state, decision_hist, j+1)\n",
    "            loss += (1. - self.attribute_coef) * self.cls_loss(classification, labels)\n",
    "            all_classifications.append(classification)\n",
    "\n",
    "            self.update_unique_attributes(self.saved_attribute_selection.unique(), j)\n",
    "\n",
    "            if self.model_type == 'xoc' and self.attribute_coef > 0.:\n",
    "                chosen_attribtutes = self.saved_attribute_selection\n",
    "                attribute_logits = self.attribute_logits\n",
    "\n",
    "                attribute_target = self.attribute_mtx[labels, :].gather(1, chosen_attribtutes.unsqueeze(1)).squeeze()\n",
    "                attribute_pred = attribute_logits.gather(1, chosen_attribtutes.unsqueeze(1)).squeeze()\n",
    "                loss += self.attribute_coef * self.attr_loss(attribute_pred,\n",
    "                                                             attribute_target)\n",
    "                \n",
    "\n",
    "\n",
    "                attribute_pred_bin = (attribute_pred > 0.).long()\n",
    "                self.update_attr_preds((attribute_pred_bin == attribute_target).sum().item(), j)\n",
    "\n",
    "                \n",
    "            if keep_tree_stats:\n",
    "                attribute_pred = self.attribute_logits.gather(1, self.saved_attribute_selection.unsqueeze(1)).squeeze()\n",
    "                self.update_tree_stats(self.saved_attribute_selection, (attribute_pred > 0.).long(),labels, j)\n",
    "\n",
    "                # all_chosen_attr.append(self.saved_attribute_selection)\n",
    "                all_attribute_preds.append((attribute_pred > 0.).long())\n",
    "\n",
    "            j += 1\n",
    "        \n",
    "            all_chosen_attr.append(self.saved_attribute_selection)\n",
    "\n",
    "        self.tmp_saved_chosen_attr = torch.stack(all_chosen_attr, dim=1)\n",
    "\n",
    "        if keep_tree_stats:\n",
    "            self.tmp_saved_cls = torch.stack(all_classifications, dim=1)\n",
    "            # self.tmp_saved_chosen_attr = torch.stack(all_chosen_attr, dim=1)\n",
    "            self.tmp_saved_attr_pred = torch.stack(all_attribute_preds, dim=1)\n",
    "        # else:\n",
    "        #     self.tmp_saved_chosen_attr = None\n",
    "\n",
    "        loss = loss / self.max_iters\n",
    "\n",
    "\n",
    "        ####################################\n",
    "        # if binary_features.size(0)==64:\n",
    "            # self.used_attributes_list.append(self.tmp_saved_chosen_attr)\n",
    "            # self.certain_attrs.append(certain_decisions)\n",
    "            # chosen_attributes_binary = torch.tensor([1 if x in [int(attr) for attr in self.tmp_saved_chosen_attr[:,x]] else 0 for x in range(312)], device=device)\n",
    "            # self.used_attributes_list.append(chosen_attributes_binary)\n",
    "            # self.sigmas_list.append(sigmas_all)\n",
    "        ####################################\n",
    "        return all_classifications, loss, self.tmp_saved_chosen_attr\n",
    "\n",
    "    def forward(self, images, labels, keep_tree_stats=False):\n",
    "        classification, loss, chosen_attribtutes = self.tree_rollout(images, labels, keep_tree_stats)\n",
    "        # classification, loss, chosen_attributes = self.tree_rollout(images, labels, keep_tree_stats)\n",
    "\n",
    "        return classification, loss#, chosen_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "After defining our models, we can continue by training it. For this, we first set some hyperparameters and then continue by defining a trainer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake input arguments\n",
    "model_type = 'xoc'\n",
    "cnn_type = 'uncertain'\n",
    "# cnn_type = 'resnet'\n",
    "attribute_coef = 0.2\n",
    "# attribute_size = 85#312 #1024\n",
    "# attribute_size = 312 #1024\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0\n",
    "step_size = 50\n",
    "num_epochs = 2\n",
    "max_iters = 10\n",
    "hidden_size = 1000\n",
    "cnn_out_size = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, dataloaders, num_epochs, device, log_freq, log_path): # todo remove stats dict\n",
    "\n",
    "        self.model = model\n",
    "        self.dataloaders = dataloaders\n",
    "        self.num_epochs = num_epochs\n",
    "        self.device = device\n",
    "        self.log_freq = log_freq\n",
    "        self.log_path = log_path\n",
    "        \n",
    "        self.logger = SummaryWriter(self.log_path)\n",
    "\n",
    "        #### TODO remove this workaround and use tensorboard\n",
    "#         with open('/content/drive/My Drive/rdtc/data/' + 'logs/' + 'stats_dict.json') as json_file:\n",
    "#             self.stats_dict = json.load(json_file)\n",
    "        \n",
    "        self.classifications_dict = {'correct':{'num':0, 'uncertainty':0},\n",
    "                                     'incorrect':{'num':0, 'uncertainty':0}}\n",
    "\n",
    "        self.uncertainty_stats = {'epoch_{}'.format(epoch):{'used_attributes':[],'sigmas':[], 'num_attrs_discarded':0} for epoch in range(num_epochs+2)}\n",
    "        self.mean_attr_accs = []\n",
    "        self.mean_drop_ratio = []\n",
    "        self.mean_sigmas = []\n",
    "        ############\n",
    "\n",
    "    def train(self):\n",
    "        self.model.phase = 'train'\n",
    "        self.train_model(self.dataloaders['train'])\n",
    "\n",
    "    def test(self):\n",
    "        self.model.phase = 'test'\n",
    "        self.test_model(self.dataloaders['test'], 'test', None, hard=False) # was: hard=True\n",
    "        self.model.phase = 'train'\n",
    "        return self.model.label_stats, self.model.selection_stats\n",
    "\n",
    "    def topk_correct(self, output, target, topk=(1,)):\n",
    "        maxk = max(topk)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        target_masks = []\n",
    "        target_cnt = []\n",
    "        for i in range(self.model.num_classes):\n",
    "            target_masks.append((target == i).unsqueeze(0))\n",
    "            target_cnt.append(target_masks[i].sum().item())\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = [(correct[:k] * tm).view(-1).float().sum(0, keepdim=True).item() for tm in target_masks]\n",
    "            res.append(np.array(correct_k))\n",
    "        return res, np.array(target_cnt)\n",
    "\n",
    "    def log_stats(self, phase, epoch, epoch_stats, hist_stats,\n",
    "                  unique_attr_stats, attr_acc):\n",
    "        for k in range(len(epoch_stats[0])):\n",
    "        # for k in range(1):\n",
    "\n",
    "            self.logger.add_scalar('Top1Accuracy{}/{}'.format(k+1, phase), epoch_stats[0][k], epoch)\n",
    "            # self.logger.add_scalar('Top5Accuracy{}/{}'.format(k+1, phase), epoch_stats[1][k], epoch)\n",
    "            # self.logger.add_scalar('Top1MeanClassAccuracy{}/{}'.format(k+1, phase), epoch_stats[3][k], epoch)\n",
    "            # self.logger.add_scalar('Top5MeanClassAccuracy{}/{}'.format(k+1, phase), epoch_stats[4][k], epoch)\n",
    "            # if unique_attr_stats is not None:\n",
    "            #     self.logger.add_scalar('UniqueAttributes{}/{}'.format(k+1, phase), unique_attr_stats[k], epoch)\n",
    "            # if attr_acc is not None:\n",
    "            #     self.logger.add_scalar('AttributeAccuracy{}/{}'.format(k+1, phase), attr_acc[k], epoch)\n",
    "        self.logger.add_scalar('Loss/'+phase, epoch_stats[2], epoch)\n",
    "\n",
    "        if hist_stats is not None:\n",
    "            for name, data in hist_stats.items():\n",
    "                data = torch.cat(data, dim=0).flatten()\n",
    "                if name.startswith('SelectionHard'):\n",
    "                    bins = self.model.attribute_size\n",
    "                elif name.startswith('AttributesHard'):\n",
    "                    bins = self.model.decision_size\n",
    "                else:\n",
    "                    bins = 'tensorflow'\n",
    "                self.logger.add_histogram(name, data, epoch, bins=bins)\n",
    "\n",
    "    def test_model(self, data_loader, phase, epoch, hard=False):\n",
    "        # Test the Model\n",
    "        self.model.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\n",
    "        n_stats = self.model.max_iters if hasattr(self.model, 'max_iters') else 1\n",
    "        correct_1 = np.zeros((n_stats, self.model.num_classes))\n",
    "        correct_5 = np.zeros((n_stats, self.model.num_classes))\n",
    "        total = 0\n",
    "        total_cnt = np.zeros((1, self.model.num_classes))\n",
    "        total_loss = 0\n",
    "\n",
    "        if isinstance(self.model, OC):\n",
    "            self.model.reset_stats()\n",
    "            if hard:\n",
    "                self.model.init_tree_stats()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for idx, data in enumerate(data_loader):\n",
    "                if len(data) == 2:\n",
    "                    images, labels = data\n",
    "                    attributes = None\n",
    "                else:\n",
    "                    images, labels, attributes = data\n",
    "                    #attributes = attributes.to(self.device)\n",
    "                images = images.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                classification, loss = self.model(images, labels, hard)\n",
    "                #####################################\n",
    "\n",
    "                #####################################\n",
    "\n",
    "                # Collect stats\n",
    "                total_loss += loss.item()\n",
    "                total += labels.size(0)\n",
    "                for k in range(len(classification)):\n",
    "                    ######################\n",
    "                    # print(classification[k].data)\n",
    "                    # print(labels[k])\n",
    "                    # print()\n",
    "                    # print(classification[k].data.size())\n",
    "                    # print(labels.size())\n",
    "                    # print()\n",
    "                    # values, indices = torch.max(classification[k], -1)\n",
    "                    # for i in range(classification[k].size(0)):\n",
    "\n",
    "                        # print(indices[i].item(),labels[i].item())\n",
    "                        # if indices[i].item()==labels[i].item():\n",
    "                        #     self.classifications_dict['correct']+=1\n",
    "                        # if indices[i].item()!=labels[i].item():\n",
    "                        #     self.classifications_dict['incorrect']+=1              \n",
    "          \n",
    "                    ######################\n",
    "                    ctopk, target_cnt = self.topk_correct(classification[k].data, labels, (1, 5))\n",
    "                    c1, c5 = ctopk\n",
    "                    # print(target_cnt)\n",
    "                    correct_1[k] += c1\n",
    "                    correct_5[k] += c5\n",
    "                total_cnt[0] += target_cnt\n",
    "\n",
    "                \n",
    "                # if classification[-1].size(0)==64:\n",
    "                #     values, indices = torch.max(classification[-1], -1)\n",
    "                #     image_features_ = self.model.cnn(images)\n",
    "                #     image_features_ = image_features_.view(image_features_.size(0), -1)\n",
    "                #     # self.model.binary_features = self.model.binary_features.train()\n",
    "                #     sigmas = self.model.get_attribute_uncertainty_batch(image_features_, batch_size=64)\n",
    "                #     sigmas.cuda()\n",
    "                #     # self.model.binary_features = self.model.binary_features.eval()\n",
    "                #     _, _, attributes_used = self.model.tree_rollout(images, labels, True)\n",
    "                #     for i in range(classification[-1].size(0)): # iterate over batch\n",
    "                #         chosen_attributes_binary = torch.tensor([1 if x in [int(attr) for attr in attributes_used[i]] else 0 for x in range(624)], device=device)\n",
    "                #         self.model.used_attributes_list.append(chosen_attributes_binary)\n",
    "                #         # chosen_attributes_binary.cuda()\n",
    "                #         used_sigmas = sigmas[i] * chosen_attributes_binary\n",
    "                #         if indices[i].item()==labels[i].item():\n",
    "                #             self.classifications_dict['correct']['num'] += 1\n",
    "                #             self.classifications_dict['correct']['uncertainty'] += used_sigmas.sum().item()\n",
    "                #         if indices[i].item()!=labels[i].item():\n",
    "                #             self.classifications_dict['incorrect']['num'] += 1\n",
    "                #             self.classifications_dict['incorrect']['uncertainty'] += used_sigmas.sum().item()\n",
    "                \n",
    "        # collect uncertainty stats#############################\n",
    "        # num_batches = len(model.used_attributes_list)\n",
    "\n",
    "        # attribute_zeros = torch.zeros(num_batches,624)\n",
    "        # # attrs_discarded_zeros = torch.zeros(num_batches, 624)\n",
    "        # sigma_zeros = torch.zeros(1, 624, device=device)\n",
    "        # num_attrs_discarded = 0\n",
    "        # for i in range(num_batches):\n",
    "        #     for batch_index in range(64):\n",
    "        #         example_attrs = [int(attr) for attr in model.used_attributes_list[i][batch_index]] # \n",
    "        #         attribute_zeros[i] += torch.tensor([1 if x in example_attrs else 0 for x in range(624)]) # used attrs binary\n",
    "        #         used_attrs_binary = torch.tensor([1 if x in example_attrs else 0 for x in range(624)]) # for that example\n",
    "        #         certain_attrs = model.certain_attrs[i][batch_index]\n",
    "        #         used_certain_attrs = used_attrs_binary.detach().cpu()[:312] * certain_attrs.detach().cpu()\n",
    "        #         num_attrs_discarded += torch.abs(used_attrs_binary.sum() - used_certain_attrs.sum())\n",
    "        #     sigma_zeros += model.sigmas_list[i].mean(dim=0) # add average of batch\n",
    "        #     # num_attrs_discarded += model.num_attrs_discarded_list[i]\n",
    "\n",
    "        \n",
    "        # self.uncertainty_stats['epoch_{}'.format(epoch)]['used_attributes']= attribute_zeros.sum(dim=0)\n",
    "        # self.model.used_attributes_list = []\n",
    "        # self.uncertainty_stats['epoch_{}'.format(epoch)]['sigmas'] = sigma_zeros/num_batches\n",
    "        # self.model.sigmas_list = []\n",
    "        # self.uncertainty_stats['epoch_{}'.format(epoch)]['num_attrs_discarded'] += num_attrs_discarded\n",
    "        # model.certain_attrs = []\n",
    "        # #####################################################################################\n",
    "\n",
    "        stats = [correct_1.sum(axis=1) / total, correct_5.sum(axis=1) / total, total_loss / total, (correct_1 / total_cnt).mean(axis=1), (correct_5 / total_cnt).mean(axis=1)]\n",
    "        print('Accuracy ({}), Top1: {:.2%}, Top5: {:.2%}'.format(phase, stats[0][-1], stats[1][-1]))\n",
    "        self.model.train()  # Change model to 'train' mode\n",
    "\n",
    "        unique_attr_stats = None\n",
    "        attr_acc = None\n",
    "        if epoch is not None:\n",
    "            if isinstance(self.model, OC):\n",
    "                hist_stats = self.model.get_hist_stats()\n",
    "                unique_attr_stats = self.model.get_unique_attributes()\n",
    "                if self.model.attribute_coef > 0.:\n",
    "                    attr_acc = self.model.get_attr_acc(total)\n",
    "                else:\n",
    "                    attr_acc = None\n",
    "            else:\n",
    "                hist_stats = None\n",
    "            self.log_stats(phase, epoch, stats, hist_stats, unique_attr_stats, attr_acc)\n",
    "\n",
    "        return stats[0][-1], stats, unique_attr_stats, attr_acc\n",
    "\n",
    "    def train_model(self, data_laoder):\n",
    "        max_accuracy = 0\n",
    "        max_agg_accuracy = 0\n",
    "        max_ma_accuracy = 0\n",
    "        max_ma_agg_accuracy = 0\n",
    "\n",
    "        if isinstance(self.model, OC):\n",
    "            self.model.reset_stats()\n",
    "\n",
    "        # Train the Model\n",
    "        ############### remove this workaround\n",
    "#         configuration = self.log_path.split('logs/')[1].split('/')[0]\n",
    "        # self.stats_dict[configuration]['losses'] = []\n",
    "        ###############################################\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "            #self.model.set_tau(epoch)\n",
    "            optimizer = self.model.get_optimizer()\n",
    "            n_stats = self.model.max_iters if hasattr(self.model, 'max_iters') else 1\n",
    "            correct_1 = np.zeros((n_stats, self.model.num_classes))\n",
    "            correct_5 = np.zeros((n_stats, self.model.num_classes))\n",
    "            total = 0\n",
    "            total_cnt = np.zeros((1, self.model.num_classes))\n",
    "            total_loss = 0\n",
    "\n",
    "            for i, data in enumerate(data_laoder):\n",
    "                \n",
    "                if len(data) == 2:\n",
    "                    images, labels = data\n",
    "                    attributes = None\n",
    "                else:\n",
    "                    images, labels, attributes = data\n",
    "                    #attributes = attributes.to(self.device)\n",
    "                images = images.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                classification, loss = self.model(images, labels)\n",
    "                \n",
    "\n",
    "                if loss.grad_fn is not None:\n",
    "                \n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # Collect stats\n",
    "                total_loss += loss.item()\n",
    "                total += labels.size(0)\n",
    "                for k in range(len(classification)):\n",
    "                    ctopk, target_cnt = self.topk_correct(classification[k].data, labels, (1, 5))\n",
    "                    c1, c5 = ctopk\n",
    "                    correct_1[k] += c1\n",
    "                    correct_5[k] += c5\n",
    "                total_cnt[0] += target_cnt\n",
    "\n",
    "                if (i+1) % self.log_freq == 0:\n",
    "                    print('Epoch [{}/{}], Iter [{}/{}] Loss: {:.4f}'\n",
    "                            .format(epoch+1, self.num_epochs, i+1, len(data_laoder)//images.size(0),\n",
    "                                    loss.item()))\n",
    "                    ##### TODO remove workaround and use tensurboard\n",
    "                    # self.stats_dict[configuration]['losses'].append(loss.item())\n",
    "            \n",
    "        # with open(data_path + '/logs/' + 'stats_dict.json', 'w') as json_file:\n",
    "        #     json.dump(self.stats_dict, json_file)\n",
    "        #     print('stats dict saved...')\n",
    "                    #############################################\n",
    "\n",
    "            self.model.get_scheduler().step()\n",
    "            # if isinstance(self.model, NeuralDecisionForest) or isinstance(self.model, OC):\n",
    "            #     self.model.toggle_update_schedule()\n",
    "\n",
    "            stats = [correct_1.sum(axis=1) / total, correct_5.sum(axis=1) / total, total_loss / total, (correct_1 / total_cnt).mean(axis=1), (correct_5 / total_cnt).mean(axis=1)]\n",
    "\n",
    "            if isinstance(self.model, OC):\n",
    "                hist_stats = self.model.get_hist_stats()\n",
    "                unique_attr_stats = self.model.get_unique_attributes()\n",
    "                if self.model.attribute_coef > 0.:\n",
    "                    attr_acc = self.model.get_attr_acc(total)\n",
    "                else:\n",
    "                    attr_acc = None\n",
    "            else:\n",
    "                hist_stats = None\n",
    "                unique_attr_stats = None\n",
    "                attr_acc = None\n",
    "            self.log_stats('train', epoch, stats, hist_stats, unique_attr_stats, attr_acc)\n",
    "            print('Accuracy (train), Top1: {:.2%}, Top5: {:.2%}'.format(stats[0][-1], stats[1][-1]))\n",
    "            self.model.phase = 'test'\n",
    "            val_accuracy, val_stats, _, _ = self.test_model(self.dataloaders['val'],\n",
    "                                                            'val', epoch+1)\n",
    "            val_agg_accuracy = val_stats[0].sum()\n",
    "            val_ma_accuracy = val_stats[3][-1]\n",
    "            val_ma_agg_accuracy = val_stats[3].sum()\n",
    "\n",
    "            # reset model stats\n",
    "            model.sigmas_list = []\n",
    "            model.used_attributes_list = []\n",
    "            model.attribute_accuracies = []\n",
    "            model.drop_ratios = []\n",
    "#             self.model.phase = 'test'\n",
    "            _, test_stats, unique_attr_stats, attr_acc = self.test_model(self.dataloaders['test'], 'test', epoch+1)\n",
    "            \n",
    "#             if model.drop_ratios[0].size(0) == 64:\n",
    "            self.mean_drop_ratio.append(sum(model.drop_ratios)/(len(model.drop_ratios)+1))\n",
    "            mean_attribute_accuracy = sum(model.attribute_accuracies)/len(model.attribute_accuracies)\n",
    "            self.mean_sigmas.append(sum(model.mean_sigmas)/(len(model.mean_sigmas)+1))\n",
    "            self.mean_attr_accs.append(mean_attribute_accuracy)\n",
    "#             print(mean_attribute_accuracy)\n",
    "#             print()\n",
    "            # # collect uncertainty stats\n",
    "            # self.uncertainty_stats['epoch_{}'.format(epoch)]['used_attributes']=self.model.used_attributes_list\n",
    "            # self.model.used_attributes_list = []\n",
    "            # self.uncertainty_stats['epoch_{}'.format(epoch)]['sigmas']=self.model.sigmas_list\n",
    "            # self.model.sigmas_list = []           \n",
    "            \n",
    "            \n",
    "            \n",
    "            self.model.phase = 'train'\n",
    "            if val_accuracy > max_accuracy:\n",
    "                max_accuracy = val_accuracy\n",
    "#                 self.save_model('best', test_stats, 0, unique_attr_stats, attr_acc, epoch)\n",
    "            if val_ma_accuracy > max_ma_accuracy:\n",
    "                max_ma_accuracy = val_ma_accuracy\n",
    "#                 self.save_model('best_ma', test_stats, 3, unique_attr_stats, attr_acc, epoch)\n",
    "            if val_agg_accuracy > max_agg_accuracy and isinstance(self.model, OC):\n",
    "                max_agg_accuracy = val_agg_accuracy\n",
    "                self.save_model('best_agg', test_stats, 0, unique_attr_stats, attr_acc, epoch)\n",
    "            if val_ma_agg_accuracy > max_ma_agg_accuracy and isinstance(self.model, OC):\n",
    "                max_ma_agg_accuracy = val_ma_agg_accuracy\n",
    "#                 self.save_model('best_ma_agg', test_stats, 3, unique_attr_stats, attr_acc, epoch)\n",
    "\n",
    "            self.save_model('latest', test_stats, 0, unique_attr_stats, attr_acc, epoch)\n",
    "        # with open(data_path + '/logs/' + 'stats_dict.json', 'w') as json_file:\n",
    "        #     json.dump(self.stats_dict, json_file)\n",
    "        #     print('stats dict saved...')\n",
    "\n",
    "\n",
    "    def write_stats_file(self, stats_list, name, epoch, is_float=True):\n",
    "        fstr = '{:.2f}' if is_float else '{}'\n",
    "        with open(os.path.join(self.log_path, '{}.txt'.format(name)), 'a') as f:\n",
    "            acc_str = [fstr.format(100*c1 if is_float else c1) for c1 in stats_list]\n",
    "            f.write('{} '.format(epoch) + ' '.join(acc_str))\n",
    "            f.write('\\n')\n",
    "\n",
    "    def save_model(self, name, test_stats, stats_idx, unique_attr_stats, attr_acc, epoch):\n",
    "        torch.save(self.model.state_dict(),\n",
    "                   os.path.join(self.log_path, '{}.pth'.format(name)))\n",
    "        \n",
    "        # save state dict for vision model separately\n",
    "        torch.save(self.model.cnn.state_dict(), os.path.join(self.log_path, '{}_vision_model.pth'.format(name)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.write_stats_file(test_stats[stats_idx], name, epoch)\n",
    "\n",
    "        # if isinstance(self.model, NeuralDecisionForest):\n",
    "        #     _, test_stats_hard, _, _ = self.test_model(self.dataloaders['test'],\n",
    "        #                                                'test', epoch+1, hard=True)\n",
    "        #     self.write_stats_file(test_stats_hard[stats_idx], name+'_hard', epoch)\n",
    "\n",
    "        if unique_attr_stats is not None:\n",
    "            self.write_stats_file(unique_attr_stats, name+'_uniqattr', epoch,\n",
    "                                  is_float=False)\n",
    "\n",
    "        if attr_acc is not None:\n",
    "            self.write_stats_file(attr_acc, name+'_attracc', epoch)\n",
    "\n",
    "        if name != 'latest':\n",
    "            print('Saved {} model'.format(name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instance our models..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the configuraion parameters of our dataset\n",
    "in_channels, cnn_out_size, log_freq = get_dataset_config(dataset, 'cnn', 40 )\n",
    "cnn_out_size = 2048\n",
    "\n",
    "# initiate the observer classifier model\n",
    "model = OC('xoc', len(classes), 'dropoutcnn', in_channels,\n",
    "            cnn_out_size, dataset, 3, 40, # 13 worked\n",
    "            attribute_size, attribute_mtx, attribute_coef,\n",
    "            hidden_size, tau_initial=5,\n",
    "            use_pretrained=False, shallow=False)\n",
    "\n",
    "# load pretrained resnet backbone\n",
    "model.cnn = models.resnet152(pretrained=False)\n",
    "# model.cnn.fc = nn.Linear(model.cnn.fc.in_features, torch.load('/home/swezel/projects/urdtc/pretrained/cub_resnet152.pkl')['fc.weight'].size(0))\n",
    "# model.cnn.load_state_dict(torch.load('/home/swezel/projects/urdtc/pretrained/cub_resnet152.pkl'))\n",
    "model.cnn.fc = nn.Linear(model.cnn.fc.in_features, torch.load('/home/swezel/projects/urdtc/pretrained/{}_resnet152.pkl'.format(dataset))['fc.weight'].size(0))\n",
    "model.cnn.load_state_dict(torch.load('/home/swezel/projects/urdtc/pretrained/{}_resnet152.pkl'.format(dataset)))\n",
    "\n",
    "\n",
    "model.cnn.fc = nn.Identity()\n",
    "\n",
    "# set attribute head\n",
    "model.binary_features = nn.Sequential(nn.BatchNorm1d(cnn_out_size),\n",
    "                                        nn.Linear(cnn_out_size, hidden_size),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                                        nn.BatchNorm1d(hidden_size),\n",
    "                                        nn.Dropout(0.5, inplace=False), # dropout after batchnorm\n",
    "                                        nn.Linear(hidden_size, hidden_size),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                                        nn.BatchNorm1d(hidden_size),\n",
    "                                        nn.Dropout(0.5, inplace=False),\n",
    "                                        nn.Linear(hidden_size, attribute_size * 2))\n",
    "\n",
    "model.binary_features.tau = nn.Parameter(torch.tensor([5], dtype=torch.float), requires_grad=True)\n",
    "model.to(device);\n",
    "\n",
    "# freeze resnet backbone for faster training but make all other weights trainable\n",
    "for param in model.lstm.parameters():\n",
    "    param.requires_grad=True\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad=True\n",
    "for param in model.feature_selection.parameters():\n",
    "    param.requires_grad=True\n",
    "for param in model.binary_features.parameters():\n",
    "    param.requires_grad=True\n",
    "for param in model.pre_lstm.parameters():\n",
    "    param.requires_grad=True\n",
    "    \n",
    "for param in model.cnn.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.cnn.fc.parameters():\n",
    "    param.requires_grad=True\n",
    "    \n",
    "# initiate optimizers\n",
    "tree_params, cnn_params = model.get_param_groups()\n",
    "tree_optimizer = torch.optim.Adam(tree_params, lr = learning_rate, weight_decay=weight_decay)\n",
    "cnn_optimizer = torch.optim.Adam(cnn_params, lr = learning_rate, weight_decay=weight_decay)\n",
    "optimizer = [tree_optimizer, cnn_optimizer]\n",
    "# and schedulers\n",
    "tree_scheduler = torch.optim.lr_scheduler.StepLR(tree_optimizer, step_size=step_size, gamma=0.1)\n",
    "cnn_scheduler = torch.optim.lr_scheduler.StepLR(cnn_optimizer, step_size=step_size, gamma=0.1)\n",
    "scheduler = [tree_scheduler, cnn_scheduler]\n",
    "\n",
    "model.set_optimizer(optimizer)\n",
    "# model.init_tree_stats()\n",
    "model.set_scheduler(scheduler)\n",
    "##hook\n",
    "# torch.load('/home/swezel/projects/urdtc/pretrained/awa2_resnet152.pkl')\n",
    "\n",
    "# model.strategy = 'randRDTC'\n",
    "# model.strategy = 'remRDTC'\n",
    "model.strategy = 'extRDTC'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and finally train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-663b28cfa507>:206: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  outputs[i] = F.softmax(self.binary_features(image_features))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/80], Iter [10/1] Loss: 4.4029\n",
      "Epoch [1/80], Iter [20/1] Loss: 4.4271\n",
      "Epoch [1/80], Iter [30/1] Loss: 4.4248\n",
      "Epoch [1/80], Iter [40/1] Loss: 4.4150\n",
      "Epoch [1/80], Iter [50/1] Loss: 4.3807\n",
      "Epoch [1/80], Iter [60/1] Loss: 4.3423\n",
      "Epoch [1/80], Iter [70/1] Loss: 4.1476\n",
      "Epoch [1/80], Iter [80/1] Loss: 4.1595\n",
      "Accuracy (train), Top1: 1.76%, Top5: 7.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-663b28cfa507>:213: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  outputs[i] = F.softmax(self.binary_features(image_features))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (val), Top1: 3.51%, Top5: 13.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-18d61168ba44>:188: RuntimeWarning: invalid value encountered in true_divide\n",
      "  stats = [correct_1.sum(axis=1) / total, correct_5.sum(axis=1) / total, total_loss / total, (correct_1 / total_cnt).mean(axis=1), (correct_5 / total_cnt).mean(axis=1)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (test), Top1: 3.24%, Top5: 12.86%\n",
      "Saved best_agg model\n",
      "Epoch [2/80], Iter [10/1] Loss: 3.9266\n",
      "Epoch [2/80], Iter [20/1] Loss: 3.7172\n",
      "Epoch [2/80], Iter [30/1] Loss: 3.5183\n",
      "Epoch [2/80], Iter [40/1] Loss: 3.6883\n",
      "Epoch [2/80], Iter [50/1] Loss: 3.9455\n",
      "Epoch [2/80], Iter [60/1] Loss: 3.5929\n",
      "Epoch [2/80], Iter [70/1] Loss: 3.4773\n",
      "Epoch [2/80], Iter [80/1] Loss: 3.3423\n",
      "Accuracy (train), Top1: 8.17%, Top5: 28.25%\n",
      "Accuracy (val), Top1: 10.02%, Top5: 30.05%\n",
      "Accuracy (test), Top1: 9.63%, Top5: 30.31%\n",
      "Saved best_agg model\n",
      "Epoch [3/80], Iter [10/1] Loss: 3.2324\n",
      "Epoch [3/80], Iter [20/1] Loss: 3.3342\n",
      "Epoch [3/80], Iter [30/1] Loss: 3.1850\n",
      "Epoch [3/80], Iter [40/1] Loss: 3.1746\n",
      "Epoch [3/80], Iter [50/1] Loss: 3.1770\n",
      "Epoch [3/80], Iter [60/1] Loss: 2.7883\n",
      "Epoch [3/80], Iter [70/1] Loss: 3.1762\n",
      "Epoch [3/80], Iter [80/1] Loss: 2.8182\n",
      "Accuracy (train), Top1: 22.02%, Top5: 54.53%\n",
      "Accuracy (val), Top1: 20.53%, Top5: 51.42%\n",
      "Accuracy (test), Top1: 19.71%, Top5: 50.05%\n",
      "Saved best_agg model\n",
      "Epoch [4/80], Iter [10/1] Loss: 2.7791\n",
      "Epoch [4/80], Iter [20/1] Loss: 2.7333\n",
      "Epoch [4/80], Iter [30/1] Loss: 2.8183\n",
      "Epoch [4/80], Iter [40/1] Loss: 2.9096\n",
      "Epoch [4/80], Iter [50/1] Loss: 2.6696\n",
      "Epoch [4/80], Iter [60/1] Loss: 2.6159\n",
      "Epoch [4/80], Iter [70/1] Loss: 2.6304\n",
      "Epoch [4/80], Iter [80/1] Loss: 2.6983\n",
      "Accuracy (train), Top1: 33.09%, Top5: 68.95%\n",
      "Accuracy (val), Top1: 32.39%, Top5: 67.11%\n",
      "Accuracy (test), Top1: 30.70%, Top5: 66.69%\n",
      "Saved best_agg model\n",
      "Epoch [5/80], Iter [10/1] Loss: 2.3973\n",
      "Epoch [5/80], Iter [20/1] Loss: 2.4775\n",
      "Epoch [5/80], Iter [30/1] Loss: 2.6616\n",
      "Epoch [5/80], Iter [40/1] Loss: 2.6278\n",
      "Epoch [5/80], Iter [50/1] Loss: 2.5390\n",
      "Epoch [5/80], Iter [60/1] Loss: 2.7151\n",
      "Epoch [5/80], Iter [70/1] Loss: 2.4999\n",
      "Epoch [5/80], Iter [80/1] Loss: 2.6111\n",
      "Accuracy (train), Top1: 42.59%, Top5: 75.89%\n",
      "Accuracy (val), Top1: 44.24%, Top5: 78.46%\n",
      "Accuracy (test), Top1: 43.96%, Top5: 77.39%\n",
      "Saved best_agg model\n",
      "Epoch [6/80], Iter [10/1] Loss: 2.6334\n",
      "Epoch [6/80], Iter [20/1] Loss: 2.3123\n",
      "Epoch [6/80], Iter [30/1] Loss: 2.3202\n",
      "Epoch [6/80], Iter [40/1] Loss: 2.2086\n",
      "Epoch [6/80], Iter [50/1] Loss: 2.5532\n",
      "Epoch [6/80], Iter [60/1] Loss: 2.4008\n",
      "Epoch [6/80], Iter [70/1] Loss: 2.2174\n",
      "Epoch [6/80], Iter [80/1] Loss: 2.4670\n",
      "Accuracy (train), Top1: 48.06%, Top5: 80.95%\n",
      "Accuracy (val), Top1: 55.26%, Top5: 82.64%\n",
      "Accuracy (test), Top1: 49.19%, Top5: 82.64%\n",
      "Saved best_agg model\n",
      "Epoch [7/80], Iter [10/1] Loss: 1.9795\n",
      "Epoch [7/80], Iter [20/1] Loss: 2.3580\n",
      "Epoch [7/80], Iter [30/1] Loss: 2.0674\n",
      "Epoch [7/80], Iter [40/1] Loss: 2.0657\n",
      "Epoch [7/80], Iter [50/1] Loss: 2.2402\n",
      "Epoch [7/80], Iter [60/1] Loss: 2.0749\n",
      "Epoch [7/80], Iter [70/1] Loss: 2.0594\n",
      "Epoch [7/80], Iter [80/1] Loss: 2.1901\n",
      "Accuracy (train), Top1: 55.72%, Top5: 84.65%\n",
      "Accuracy (val), Top1: 58.26%, Top5: 86.14%\n",
      "Accuracy (test), Top1: 52.93%, Top5: 85.19%\n",
      "Saved best_agg model\n",
      "Epoch [8/80], Iter [10/1] Loss: 2.0545\n",
      "Epoch [8/80], Iter [20/1] Loss: 2.1001\n",
      "Epoch [8/80], Iter [30/1] Loss: 1.9591\n",
      "Epoch [8/80], Iter [40/1] Loss: 1.8534\n",
      "Epoch [8/80], Iter [50/1] Loss: 2.1820\n",
      "Epoch [8/80], Iter [60/1] Loss: 2.7161\n",
      "Epoch [8/80], Iter [70/1] Loss: 1.9621\n",
      "Epoch [8/80], Iter [80/1] Loss: 2.0519\n",
      "Accuracy (train), Top1: 58.09%, Top5: 84.87%\n",
      "Accuracy (val), Top1: 58.93%, Top5: 85.64%\n",
      "Accuracy (test), Top1: 52.52%, Top5: 84.36%\n",
      "Epoch [9/80], Iter [10/1] Loss: 1.9429\n",
      "Epoch [9/80], Iter [20/1] Loss: 2.0070\n",
      "Epoch [9/80], Iter [30/1] Loss: 2.0916\n",
      "Epoch [9/80], Iter [40/1] Loss: 2.1022\n",
      "Epoch [9/80], Iter [50/1] Loss: 2.2436\n",
      "Epoch [9/80], Iter [60/1] Loss: 2.0185\n",
      "Epoch [9/80], Iter [70/1] Loss: 1.8971\n",
      "Epoch [9/80], Iter [80/1] Loss: 2.2760\n",
      "Accuracy (train), Top1: 61.45%, Top5: 86.49%\n",
      "Accuracy (val), Top1: 67.78%, Top5: 89.15%\n",
      "Accuracy (test), Top1: 58.77%, Top5: 86.24%\n",
      "Saved best_agg model\n",
      "Epoch [10/80], Iter [10/1] Loss: 2.1280\n",
      "Epoch [10/80], Iter [20/1] Loss: 1.9758\n",
      "Epoch [10/80], Iter [30/1] Loss: 2.1759\n",
      "Epoch [10/80], Iter [40/1] Loss: 2.0245\n",
      "Epoch [10/80], Iter [50/1] Loss: 2.1757\n",
      "Epoch [10/80], Iter [60/1] Loss: 2.2119\n",
      "Epoch [10/80], Iter [70/1] Loss: 2.3305\n",
      "Epoch [10/80], Iter [80/1] Loss: 1.6917\n",
      "Accuracy (train), Top1: 64.69%, Top5: 88.01%\n",
      "Accuracy (val), Top1: 63.77%, Top5: 87.31%\n",
      "Accuracy (test), Top1: 59.35%, Top5: 86.50%\n",
      "Epoch [11/80], Iter [10/1] Loss: 1.9468\n",
      "Epoch [11/80], Iter [20/1] Loss: 2.0420\n",
      "Epoch [11/80], Iter [30/1] Loss: 1.7720\n",
      "Epoch [11/80], Iter [40/1] Loss: 2.0834\n",
      "Epoch [11/80], Iter [50/1] Loss: 1.7141\n",
      "Epoch [11/80], Iter [60/1] Loss: 1.7465\n",
      "Epoch [11/80], Iter [70/1] Loss: 1.9933\n",
      "Epoch [11/80], Iter [80/1] Loss: 1.6806\n",
      "Accuracy (train), Top1: 66.06%, Top5: 88.04%\n",
      "Accuracy (val), Top1: 63.94%, Top5: 86.48%\n",
      "Accuracy (test), Top1: 59.30%, Top5: 85.85%\n",
      "Epoch [12/80], Iter [10/1] Loss: 1.8234\n",
      "Epoch [12/80], Iter [20/1] Loss: 1.8644\n",
      "Epoch [12/80], Iter [30/1] Loss: 1.9666\n",
      "Epoch [12/80], Iter [40/1] Loss: 1.9031\n",
      "Epoch [12/80], Iter [50/1] Loss: 1.8159\n",
      "Epoch [12/80], Iter [60/1] Loss: 1.8979\n",
      "Epoch [12/80], Iter [70/1] Loss: 2.1099\n",
      "Epoch [12/80], Iter [80/1] Loss: 1.7069\n",
      "Accuracy (train), Top1: 68.01%, Top5: 88.56%\n",
      "Accuracy (val), Top1: 65.94%, Top5: 89.32%\n",
      "Accuracy (test), Top1: 60.68%, Top5: 87.14%\n",
      "Epoch [13/80], Iter [10/1] Loss: 1.8357\n",
      "Epoch [13/80], Iter [20/1] Loss: 2.0855\n",
      "Epoch [13/80], Iter [30/1] Loss: 1.8784\n",
      "Epoch [13/80], Iter [40/1] Loss: 2.0117\n",
      "Epoch [13/80], Iter [50/1] Loss: 1.8574\n",
      "Epoch [13/80], Iter [60/1] Loss: 1.8058\n",
      "Epoch [13/80], Iter [70/1] Loss: 1.7241\n",
      "Epoch [13/80], Iter [80/1] Loss: 1.9962\n",
      "Accuracy (train), Top1: 68.38%, Top5: 88.99%\n",
      "Accuracy (val), Top1: 70.78%, Top5: 90.32%\n",
      "Accuracy (test), Top1: 64.19%, Top5: 88.35%\n",
      "Saved best_agg model\n",
      "Epoch [14/80], Iter [10/1] Loss: 1.8078\n",
      "Epoch [14/80], Iter [20/1] Loss: 1.8090\n",
      "Epoch [14/80], Iter [30/1] Loss: 1.9108\n",
      "Epoch [14/80], Iter [40/1] Loss: 2.2293\n",
      "Epoch [14/80], Iter [50/1] Loss: 1.7609\n",
      "Epoch [14/80], Iter [60/1] Loss: 1.5712\n",
      "Epoch [14/80], Iter [70/1] Loss: 1.7438\n",
      "Epoch [14/80], Iter [80/1] Loss: 1.7210\n",
      "Accuracy (train), Top1: 69.60%, Top5: 89.16%\n",
      "Accuracy (val), Top1: 73.79%, Top5: 91.82%\n",
      "Accuracy (test), Top1: 62.69%, Top5: 88.63%\n",
      "Saved best_agg model\n",
      "Epoch [15/80], Iter [10/1] Loss: 1.4887\n",
      "Epoch [15/80], Iter [20/1] Loss: 1.8461\n",
      "Epoch [15/80], Iter [30/1] Loss: 1.6727\n",
      "Epoch [15/80], Iter [40/1] Loss: 1.6496\n",
      "Epoch [15/80], Iter [50/1] Loss: 2.0143\n",
      "Epoch [15/80], Iter [60/1] Loss: 2.3029\n",
      "Epoch [15/80], Iter [70/1] Loss: 1.6050\n",
      "Epoch [15/80], Iter [80/1] Loss: 1.8230\n",
      "Accuracy (train), Top1: 71.18%, Top5: 89.62%\n",
      "Accuracy (val), Top1: 70.62%, Top5: 90.32%\n",
      "Accuracy (test), Top1: 64.84%, Top5: 88.54%\n",
      "Epoch [16/80], Iter [10/1] Loss: 1.6936\n",
      "Epoch [16/80], Iter [20/1] Loss: 1.9858\n",
      "Epoch [16/80], Iter [30/1] Loss: 1.6805\n",
      "Epoch [16/80], Iter [40/1] Loss: 1.6927\n",
      "Epoch [16/80], Iter [50/1] Loss: 1.9558\n",
      "Epoch [16/80], Iter [60/1] Loss: 2.2964\n",
      "Epoch [16/80], Iter [70/1] Loss: 1.9924\n",
      "Epoch [16/80], Iter [80/1] Loss: 1.9763\n",
      "Accuracy (train), Top1: 70.94%, Top5: 89.56%\n",
      "Accuracy (val), Top1: 72.79%, Top5: 91.49%\n",
      "Accuracy (test), Top1: 66.98%, Top5: 89.96%\n",
      "Saved best_agg model\n",
      "Epoch [17/80], Iter [10/1] Loss: 1.7142\n",
      "Epoch [17/80], Iter [20/1] Loss: 1.5399\n",
      "Epoch [17/80], Iter [30/1] Loss: 2.0072\n",
      "Epoch [17/80], Iter [40/1] Loss: 1.5960\n",
      "Epoch [17/80], Iter [50/1] Loss: 1.6941\n",
      "Epoch [17/80], Iter [60/1] Loss: 1.6927\n",
      "Epoch [17/80], Iter [70/1] Loss: 1.7413\n",
      "Epoch [17/80], Iter [80/1] Loss: 2.1278\n",
      "Accuracy (train), Top1: 72.64%, Top5: 89.58%\n",
      "Accuracy (val), Top1: 76.46%, Top5: 89.48%\n",
      "Accuracy (test), Top1: 66.34%, Top5: 89.20%\n",
      "Saved best_agg model\n",
      "Epoch [18/80], Iter [10/1] Loss: 1.6497\n",
      "Epoch [18/80], Iter [20/1] Loss: 1.5726\n",
      "Epoch [18/80], Iter [30/1] Loss: 1.8958\n",
      "Epoch [18/80], Iter [40/1] Loss: 2.0806\n",
      "Epoch [18/80], Iter [50/1] Loss: 1.8569\n",
      "Epoch [18/80], Iter [60/1] Loss: 1.8554\n",
      "Epoch [18/80], Iter [70/1] Loss: 1.8957\n",
      "Epoch [18/80], Iter [80/1] Loss: 1.6741\n",
      "Accuracy (train), Top1: 72.99%, Top5: 89.75%\n",
      "Accuracy (val), Top1: 74.96%, Top5: 90.32%\n",
      "Accuracy (test), Top1: 66.90%, Top5: 89.78%\n",
      "Epoch [19/80], Iter [10/1] Loss: 1.7766\n",
      "Epoch [19/80], Iter [20/1] Loss: 1.6197\n",
      "Epoch [19/80], Iter [30/1] Loss: 1.5985\n",
      "Epoch [19/80], Iter [40/1] Loss: 1.6856\n",
      "Epoch [19/80], Iter [50/1] Loss: 1.6302\n",
      "Epoch [19/80], Iter [60/1] Loss: 1.7564\n",
      "Epoch [19/80], Iter [70/1] Loss: 1.7186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/80], Iter [80/1] Loss: 1.7419\n",
      "Accuracy (train), Top1: 73.90%, Top5: 89.66%\n",
      "Accuracy (val), Top1: 75.79%, Top5: 92.49%\n",
      "Accuracy (test), Top1: 66.97%, Top5: 88.66%\n",
      "Epoch [20/80], Iter [10/1] Loss: 1.5223\n",
      "Epoch [20/80], Iter [20/1] Loss: 1.5699\n",
      "Epoch [20/80], Iter [30/1] Loss: 1.9676\n",
      "Epoch [20/80], Iter [40/1] Loss: 1.6886\n",
      "Epoch [20/80], Iter [50/1] Loss: 1.4861\n",
      "Epoch [20/80], Iter [60/1] Loss: 1.7601\n",
      "Epoch [20/80], Iter [70/1] Loss: 1.6509\n",
      "Epoch [20/80], Iter [80/1] Loss: 1.5886\n",
      "Accuracy (train), Top1: 74.37%, Top5: 90.81%\n",
      "Accuracy (val), Top1: 76.63%, Top5: 90.48%\n",
      "Accuracy (test), Top1: 68.12%, Top5: 89.44%\n",
      "Epoch [21/80], Iter [10/1] Loss: 2.0324\n",
      "Epoch [21/80], Iter [20/1] Loss: 1.4076\n",
      "Epoch [21/80], Iter [30/1] Loss: 1.5311\n",
      "Epoch [21/80], Iter [40/1] Loss: 1.7351\n",
      "Epoch [21/80], Iter [50/1] Loss: 1.7237\n",
      "Epoch [21/80], Iter [60/1] Loss: 1.6700\n",
      "Epoch [21/80], Iter [70/1] Loss: 1.6845\n",
      "Epoch [21/80], Iter [80/1] Loss: 1.9432\n",
      "Accuracy (train), Top1: 74.24%, Top5: 90.60%\n",
      "Accuracy (val), Top1: 75.79%, Top5: 90.32%\n",
      "Accuracy (test), Top1: 67.93%, Top5: 89.40%\n",
      "Epoch [22/80], Iter [10/1] Loss: 1.5727\n",
      "Epoch [22/80], Iter [20/1] Loss: 1.7182\n",
      "Epoch [22/80], Iter [30/1] Loss: 1.5686\n",
      "Epoch [22/80], Iter [40/1] Loss: 1.5239\n",
      "Epoch [22/80], Iter [50/1] Loss: 1.6883\n",
      "Epoch [22/80], Iter [60/1] Loss: 1.6451\n",
      "Epoch [22/80], Iter [70/1] Loss: 1.7289\n",
      "Epoch [22/80], Iter [80/1] Loss: 1.7500\n",
      "Accuracy (train), Top1: 74.88%, Top5: 90.51%\n",
      "Accuracy (val), Top1: 77.63%, Top5: 91.32%\n",
      "Accuracy (test), Top1: 69.47%, Top5: 89.54%\n",
      "Epoch [23/80], Iter [10/1] Loss: 1.8721\n",
      "Epoch [23/80], Iter [20/1] Loss: 1.5126\n",
      "Epoch [23/80], Iter [30/1] Loss: 1.8291\n",
      "Epoch [23/80], Iter [40/1] Loss: 1.6639\n",
      "Epoch [23/80], Iter [50/1] Loss: 1.5258\n",
      "Epoch [23/80], Iter [60/1] Loss: 1.6352\n",
      "Epoch [23/80], Iter [70/1] Loss: 2.1525\n",
      "Epoch [23/80], Iter [80/1] Loss: 1.8893\n",
      "Accuracy (train), Top1: 75.66%, Top5: 90.79%\n",
      "Accuracy (val), Top1: 79.63%, Top5: 91.82%\n",
      "Accuracy (test), Top1: 68.80%, Top5: 89.47%\n",
      "Saved best_agg model\n",
      "Epoch [24/80], Iter [10/1] Loss: 1.7692\n",
      "Epoch [24/80], Iter [20/1] Loss: 1.6555\n",
      "Epoch [24/80], Iter [30/1] Loss: 1.5177\n",
      "Epoch [24/80], Iter [40/1] Loss: 1.7834\n",
      "Epoch [24/80], Iter [50/1] Loss: 1.6493\n",
      "Epoch [24/80], Iter [60/1] Loss: 1.6325\n",
      "Epoch [24/80], Iter [70/1] Loss: 1.4389\n",
      "Epoch [24/80], Iter [80/1] Loss: 1.8567\n",
      "Accuracy (train), Top1: 76.20%, Top5: 90.88%\n",
      "Accuracy (val), Top1: 78.30%, Top5: 89.65%\n",
      "Accuracy (test), Top1: 70.61%, Top5: 90.63%\n",
      "Epoch [25/80], Iter [10/1] Loss: 1.4902\n",
      "Epoch [25/80], Iter [20/1] Loss: 1.4118\n",
      "Epoch [25/80], Iter [30/1] Loss: 2.1389\n",
      "Epoch [25/80], Iter [40/1] Loss: 1.4617\n",
      "Epoch [25/80], Iter [50/1] Loss: 1.6222\n",
      "Epoch [25/80], Iter [60/1] Loss: 1.4783\n",
      "Epoch [25/80], Iter [70/1] Loss: 1.5623\n",
      "Epoch [25/80], Iter [80/1] Loss: 1.6700\n",
      "Accuracy (train), Top1: 76.16%, Top5: 90.18%\n",
      "Accuracy (val), Top1: 78.13%, Top5: 88.48%\n",
      "Accuracy (test), Top1: 70.47%, Top5: 90.13%\n",
      "Epoch [26/80], Iter [10/1] Loss: 1.5586\n",
      "Epoch [26/80], Iter [20/1] Loss: 1.4855\n",
      "Epoch [26/80], Iter [30/1] Loss: 1.5225\n",
      "Epoch [26/80], Iter [40/1] Loss: 1.4405\n",
      "Epoch [26/80], Iter [50/1] Loss: 1.6443\n",
      "Epoch [26/80], Iter [60/1] Loss: 1.5688\n",
      "Epoch [26/80], Iter [70/1] Loss: 1.5776\n",
      "Epoch [26/80], Iter [80/1] Loss: 1.3908\n",
      "Accuracy (train), Top1: 77.26%, Top5: 90.82%\n",
      "Accuracy (val), Top1: 76.96%, Top5: 91.15%\n",
      "Accuracy (test), Top1: 69.35%, Top5: 89.80%\n",
      "Epoch [27/80], Iter [10/1] Loss: 1.5795\n",
      "Epoch [27/80], Iter [20/1] Loss: 1.5594\n",
      "Epoch [27/80], Iter [30/1] Loss: 1.8134\n",
      "Epoch [27/80], Iter [40/1] Loss: 1.7937\n",
      "Epoch [27/80], Iter [50/1] Loss: 1.5659\n",
      "Epoch [27/80], Iter [60/1] Loss: 1.7322\n",
      "Epoch [27/80], Iter [70/1] Loss: 1.8274\n",
      "Epoch [27/80], Iter [80/1] Loss: 1.4586\n",
      "Accuracy (train), Top1: 77.44%, Top5: 90.60%\n",
      "Accuracy (val), Top1: 80.97%, Top5: 92.82%\n",
      "Accuracy (test), Top1: 69.73%, Top5: 90.04%\n",
      "Epoch [28/80], Iter [10/1] Loss: 1.8441\n",
      "Epoch [28/80], Iter [20/1] Loss: 1.7262\n",
      "Epoch [28/80], Iter [30/1] Loss: 1.7378\n",
      "Epoch [28/80], Iter [40/1] Loss: 1.5386\n",
      "Epoch [28/80], Iter [50/1] Loss: 1.3965\n",
      "Epoch [28/80], Iter [60/1] Loss: 1.8085\n",
      "Epoch [28/80], Iter [70/1] Loss: 1.5074\n",
      "Epoch [28/80], Iter [80/1] Loss: 1.5218\n",
      "Accuracy (train), Top1: 78.05%, Top5: 91.12%\n",
      "Accuracy (val), Top1: 74.46%, Top5: 88.98%\n",
      "Accuracy (test), Top1: 67.78%, Top5: 88.66%\n",
      "Epoch [29/80], Iter [10/1] Loss: 1.4301\n",
      "Epoch [29/80], Iter [20/1] Loss: 1.6252\n",
      "Epoch [29/80], Iter [30/1] Loss: 1.4931\n",
      "Epoch [29/80], Iter [40/1] Loss: 1.5183\n",
      "Epoch [29/80], Iter [50/1] Loss: 1.3044\n",
      "Epoch [29/80], Iter [60/1] Loss: 1.6762\n",
      "Epoch [29/80], Iter [70/1] Loss: 1.5464\n",
      "Epoch [29/80], Iter [80/1] Loss: 1.8925\n",
      "Accuracy (train), Top1: 76.61%, Top5: 90.38%\n",
      "Accuracy (val), Top1: 79.30%, Top5: 91.32%\n",
      "Accuracy (test), Top1: 70.21%, Top5: 90.54%\n",
      "Epoch [30/80], Iter [10/1] Loss: 1.4898\n",
      "Epoch [30/80], Iter [20/1] Loss: 1.7411\n",
      "Epoch [30/80], Iter [30/1] Loss: 1.6667\n",
      "Epoch [30/80], Iter [40/1] Loss: 1.4880\n",
      "Epoch [30/80], Iter [50/1] Loss: 1.3688\n",
      "Epoch [30/80], Iter [60/1] Loss: 1.6890\n",
      "Epoch [30/80], Iter [70/1] Loss: 1.4617\n",
      "Epoch [30/80], Iter [80/1] Loss: 1.7111\n",
      "Accuracy (train), Top1: 78.98%, Top5: 92.20%\n",
      "Accuracy (val), Top1: 79.13%, Top5: 88.81%\n",
      "Accuracy (test), Top1: 70.11%, Top5: 89.94%\n",
      "Epoch [31/80], Iter [10/1] Loss: 1.4430\n",
      "Epoch [31/80], Iter [20/1] Loss: 1.6139\n",
      "Epoch [31/80], Iter [30/1] Loss: 1.5498\n",
      "Epoch [31/80], Iter [40/1] Loss: 1.3958\n",
      "Epoch [31/80], Iter [50/1] Loss: 1.7738\n",
      "Epoch [31/80], Iter [60/1] Loss: 1.4237\n",
      "Epoch [31/80], Iter [70/1] Loss: 1.5087\n",
      "Epoch [31/80], Iter [80/1] Loss: 1.4228\n",
      "Accuracy (train), Top1: 78.04%, Top5: 90.95%\n",
      "Accuracy (val), Top1: 81.80%, Top5: 92.32%\n",
      "Accuracy (test), Top1: 70.59%, Top5: 89.80%\n",
      "Saved best_agg model\n",
      "Epoch [32/80], Iter [10/1] Loss: 1.3926\n",
      "Epoch [32/80], Iter [20/1] Loss: 1.4274\n",
      "Epoch [32/80], Iter [30/1] Loss: 1.3777\n",
      "Epoch [32/80], Iter [40/1] Loss: 1.3779\n",
      "Epoch [32/80], Iter [50/1] Loss: 1.2247\n",
      "Epoch [32/80], Iter [60/1] Loss: 1.3465\n",
      "Epoch [32/80], Iter [70/1] Loss: 1.3700\n",
      "Epoch [32/80], Iter [80/1] Loss: 1.5840\n",
      "Accuracy (train), Top1: 78.15%, Top5: 90.90%\n",
      "Accuracy (val), Top1: 76.63%, Top5: 90.32%\n",
      "Accuracy (test), Top1: 69.64%, Top5: 89.59%\n",
      "Epoch [33/80], Iter [10/1] Loss: 1.4791\n",
      "Epoch [33/80], Iter [20/1] Loss: 1.5756\n",
      "Epoch [33/80], Iter [30/1] Loss: 1.5786\n",
      "Epoch [33/80], Iter [40/1] Loss: 1.6608\n",
      "Epoch [33/80], Iter [50/1] Loss: 1.4569\n",
      "Epoch [33/80], Iter [60/1] Loss: 1.8716\n",
      "Epoch [33/80], Iter [70/1] Loss: 1.4054\n",
      "Epoch [33/80], Iter [80/1] Loss: 1.4700\n",
      "Accuracy (train), Top1: 78.41%, Top5: 91.18%\n",
      "Accuracy (val), Top1: 76.63%, Top5: 90.65%\n",
      "Accuracy (test), Top1: 70.06%, Top5: 90.33%\n",
      "Epoch [34/80], Iter [10/1] Loss: 1.4737\n",
      "Epoch [34/80], Iter [20/1] Loss: 1.8324\n",
      "Epoch [34/80], Iter [30/1] Loss: 1.5789\n",
      "Epoch [34/80], Iter [40/1] Loss: 1.5136\n",
      "Epoch [34/80], Iter [50/1] Loss: 1.7026\n",
      "Epoch [34/80], Iter [60/1] Loss: 1.5237\n",
      "Epoch [34/80], Iter [70/1] Loss: 1.3315\n",
      "Epoch [34/80], Iter [80/1] Loss: 1.3705\n",
      "Accuracy (train), Top1: 79.80%, Top5: 92.07%\n",
      "Accuracy (val), Top1: 78.96%, Top5: 90.15%\n",
      "Accuracy (test), Top1: 70.33%, Top5: 90.89%\n",
      "Epoch [35/80], Iter [10/1] Loss: 1.5041\n",
      "Epoch [35/80], Iter [20/1] Loss: 1.6703\n",
      "Epoch [35/80], Iter [30/1] Loss: 1.4100\n",
      "Epoch [35/80], Iter [40/1] Loss: 1.3710\n",
      "Epoch [35/80], Iter [50/1] Loss: 1.5191\n",
      "Epoch [35/80], Iter [60/1] Loss: 1.6853\n",
      "Epoch [35/80], Iter [70/1] Loss: 1.4239\n",
      "Epoch [35/80], Iter [80/1] Loss: 1.3679\n",
      "Accuracy (train), Top1: 78.98%, Top5: 91.31%\n",
      "Accuracy (val), Top1: 81.47%, Top5: 92.65%\n",
      "Accuracy (test), Top1: 72.51%, Top5: 90.51%\n",
      "Saved best_agg model\n",
      "Epoch [36/80], Iter [10/1] Loss: 1.6824\n",
      "Epoch [36/80], Iter [20/1] Loss: 1.3301\n",
      "Epoch [36/80], Iter [30/1] Loss: 1.2878\n",
      "Epoch [36/80], Iter [40/1] Loss: 1.6104\n",
      "Epoch [36/80], Iter [50/1] Loss: 1.5996\n",
      "Epoch [36/80], Iter [60/1] Loss: 1.6039\n",
      "Epoch [36/80], Iter [70/1] Loss: 1.7358\n",
      "Epoch [36/80], Iter [80/1] Loss: 1.3978\n",
      "Accuracy (train), Top1: 79.11%, Top5: 90.97%\n",
      "Accuracy (val), Top1: 80.63%, Top5: 91.32%\n",
      "Accuracy (test), Top1: 71.76%, Top5: 90.58%\n",
      "Epoch [37/80], Iter [10/1] Loss: 1.5854\n",
      "Epoch [37/80], Iter [20/1] Loss: 1.5565\n",
      "Epoch [37/80], Iter [30/1] Loss: 1.5405\n",
      "Epoch [37/80], Iter [40/1] Loss: 2.0806\n",
      "Epoch [37/80], Iter [50/1] Loss: 1.2070\n",
      "Epoch [37/80], Iter [60/1] Loss: 1.8240\n",
      "Epoch [37/80], Iter [70/1] Loss: 1.5916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/80], Iter [80/1] Loss: 1.2864\n",
      "Accuracy (train), Top1: 78.85%, Top5: 90.47%\n",
      "Accuracy (val), Top1: 80.80%, Top5: 91.99%\n",
      "Accuracy (test), Top1: 72.30%, Top5: 89.71%\n",
      "Epoch [38/80], Iter [10/1] Loss: 1.4812\n",
      "Epoch [38/80], Iter [20/1] Loss: 1.2632\n",
      "Epoch [38/80], Iter [30/1] Loss: 1.5581\n",
      "Epoch [38/80], Iter [40/1] Loss: 1.5465\n",
      "Epoch [38/80], Iter [50/1] Loss: 1.6721\n",
      "Epoch [38/80], Iter [60/1] Loss: 1.2709\n",
      "Epoch [38/80], Iter [70/1] Loss: 1.6616\n",
      "Epoch [38/80], Iter [80/1] Loss: 1.8621\n",
      "Accuracy (train), Top1: 78.76%, Top5: 90.82%\n",
      "Accuracy (val), Top1: 82.80%, Top5: 92.65%\n",
      "Accuracy (test), Top1: 71.78%, Top5: 90.23%\n",
      "Saved best_agg model\n",
      "Epoch [39/80], Iter [10/1] Loss: 1.4176\n",
      "Epoch [39/80], Iter [20/1] Loss: 1.4673\n",
      "Epoch [39/80], Iter [30/1] Loss: 1.4394\n",
      "Epoch [39/80], Iter [40/1] Loss: 1.2981\n",
      "Epoch [39/80], Iter [50/1] Loss: 1.3458\n",
      "Epoch [39/80], Iter [60/1] Loss: 1.9165\n",
      "Epoch [39/80], Iter [70/1] Loss: 1.4446\n",
      "Epoch [39/80], Iter [80/1] Loss: 1.6113\n",
      "Accuracy (train), Top1: 79.91%, Top5: 91.14%\n",
      "Accuracy (val), Top1: 80.13%, Top5: 92.32%\n",
      "Accuracy (test), Top1: 71.76%, Top5: 90.13%\n",
      "Epoch [40/80], Iter [10/1] Loss: 1.4472\n",
      "Epoch [40/80], Iter [20/1] Loss: 1.3258\n",
      "Epoch [40/80], Iter [30/1] Loss: 1.5109\n",
      "Epoch [40/80], Iter [40/1] Loss: 1.5516\n",
      "Epoch [40/80], Iter [50/1] Loss: 1.6665\n",
      "Epoch [40/80], Iter [60/1] Loss: 1.4932\n",
      "Epoch [40/80], Iter [70/1] Loss: 1.5093\n",
      "Epoch [40/80], Iter [80/1] Loss: 1.5489\n",
      "Accuracy (train), Top1: 80.06%, Top5: 91.75%\n",
      "Accuracy (val), Top1: 82.64%, Top5: 91.49%\n",
      "Accuracy (test), Top1: 71.94%, Top5: 90.01%\n",
      "Epoch [41/80], Iter [10/1] Loss: 1.4118\n",
      "Epoch [41/80], Iter [20/1] Loss: 1.5045\n",
      "Epoch [41/80], Iter [30/1] Loss: 1.5908\n",
      "Epoch [41/80], Iter [40/1] Loss: 1.5506\n",
      "Epoch [41/80], Iter [50/1] Loss: 1.3068\n",
      "Epoch [41/80], Iter [60/1] Loss: 1.2955\n",
      "Epoch [41/80], Iter [70/1] Loss: 1.2919\n",
      "Epoch [41/80], Iter [80/1] Loss: 1.6703\n",
      "Accuracy (train), Top1: 80.56%, Top5: 91.38%\n",
      "Accuracy (val), Top1: 79.47%, Top5: 91.15%\n",
      "Accuracy (test), Top1: 72.54%, Top5: 90.28%\n",
      "Epoch [42/80], Iter [10/1] Loss: 1.7987\n",
      "Epoch [42/80], Iter [20/1] Loss: 1.5432\n",
      "Epoch [42/80], Iter [30/1] Loss: 1.4255\n",
      "Epoch [42/80], Iter [40/1] Loss: 1.4932\n",
      "Epoch [42/80], Iter [50/1] Loss: 1.2144\n",
      "Epoch [42/80], Iter [60/1] Loss: 1.3938\n",
      "Epoch [42/80], Iter [70/1] Loss: 1.3605\n",
      "Epoch [42/80], Iter [80/1] Loss: 1.6407\n",
      "Accuracy (train), Top1: 80.26%, Top5: 91.36%\n",
      "Accuracy (val), Top1: 82.30%, Top5: 91.49%\n",
      "Accuracy (test), Top1: 71.78%, Top5: 90.30%\n",
      "Epoch [43/80], Iter [10/1] Loss: 1.5712\n",
      "Epoch [43/80], Iter [20/1] Loss: 1.4654\n",
      "Epoch [43/80], Iter [30/1] Loss: 1.4795\n",
      "Epoch [43/80], Iter [40/1] Loss: 1.5132\n",
      "Epoch [43/80], Iter [50/1] Loss: 1.3884\n",
      "Epoch [43/80], Iter [60/1] Loss: 1.4300\n",
      "Epoch [43/80], Iter [70/1] Loss: 1.3503\n",
      "Epoch [43/80], Iter [80/1] Loss: 1.5482\n",
      "Accuracy (train), Top1: 79.50%, Top5: 90.82%\n",
      "Accuracy (val), Top1: 83.31%, Top5: 92.49%\n",
      "Accuracy (test), Top1: 72.82%, Top5: 90.39%\n",
      "Epoch [44/80], Iter [10/1] Loss: 1.3108\n",
      "Epoch [44/80], Iter [20/1] Loss: 1.4467\n",
      "Epoch [44/80], Iter [30/1] Loss: 1.9718\n",
      "Epoch [44/80], Iter [40/1] Loss: 1.6534\n",
      "Epoch [44/80], Iter [50/1] Loss: 1.4520\n",
      "Epoch [44/80], Iter [60/1] Loss: 1.3770\n",
      "Epoch [44/80], Iter [70/1] Loss: 1.3830\n",
      "Epoch [44/80], Iter [80/1] Loss: 1.5254\n",
      "Accuracy (train), Top1: 80.19%, Top5: 91.73%\n",
      "Accuracy (val), Top1: 82.80%, Top5: 91.99%\n",
      "Accuracy (test), Top1: 73.02%, Top5: 90.46%\n",
      "Epoch [45/80], Iter [10/1] Loss: 1.6834\n",
      "Epoch [45/80], Iter [20/1] Loss: 1.0753\n",
      "Epoch [45/80], Iter [30/1] Loss: 1.5415\n",
      "Epoch [45/80], Iter [40/1] Loss: 1.4506\n",
      "Epoch [45/80], Iter [50/1] Loss: 1.1852\n",
      "Epoch [45/80], Iter [60/1] Loss: 1.6457\n",
      "Epoch [45/80], Iter [70/1] Loss: 1.3349\n",
      "Epoch [45/80], Iter [80/1] Loss: 1.6642\n",
      "Accuracy (train), Top1: 79.61%, Top5: 90.79%\n",
      "Accuracy (val), Top1: 82.64%, Top5: 91.32%\n",
      "Accuracy (test), Top1: 72.94%, Top5: 90.46%\n",
      "Epoch [46/80], Iter [10/1] Loss: 1.5835\n",
      "Epoch [46/80], Iter [20/1] Loss: 1.7387\n",
      "Epoch [46/80], Iter [30/1] Loss: 1.4989\n",
      "Epoch [46/80], Iter [40/1] Loss: 1.3945\n",
      "Epoch [46/80], Iter [50/1] Loss: 1.3985\n",
      "Epoch [46/80], Iter [60/1] Loss: 1.5301\n",
      "Epoch [46/80], Iter [70/1] Loss: 1.4303\n",
      "Epoch [46/80], Iter [80/1] Loss: 1.1477\n",
      "Accuracy (train), Top1: 80.61%, Top5: 91.01%\n",
      "Accuracy (val), Top1: 80.13%, Top5: 89.32%\n",
      "Accuracy (test), Top1: 73.08%, Top5: 90.54%\n",
      "Epoch [47/80], Iter [10/1] Loss: 1.5029\n",
      "Epoch [47/80], Iter [20/1] Loss: 1.5424\n",
      "Epoch [47/80], Iter [30/1] Loss: 1.4356\n",
      "Epoch [47/80], Iter [40/1] Loss: 1.5164\n",
      "Epoch [47/80], Iter [50/1] Loss: 1.6773\n",
      "Epoch [47/80], Iter [60/1] Loss: 1.4818\n",
      "Epoch [47/80], Iter [70/1] Loss: 1.5124\n",
      "Epoch [47/80], Iter [80/1] Loss: 1.2195\n",
      "Accuracy (train), Top1: 80.02%, Top5: 90.82%\n",
      "Accuracy (val), Top1: 80.97%, Top5: 90.98%\n",
      "Accuracy (test), Top1: 72.83%, Top5: 89.90%\n",
      "Epoch [48/80], Iter [10/1] Loss: 1.2364\n",
      "Epoch [48/80], Iter [20/1] Loss: 1.5470\n",
      "Epoch [48/80], Iter [30/1] Loss: 1.3800\n",
      "Epoch [48/80], Iter [40/1] Loss: 1.2311\n",
      "Epoch [48/80], Iter [50/1] Loss: 1.4388\n",
      "Epoch [48/80], Iter [60/1] Loss: 1.4595\n",
      "Epoch [48/80], Iter [70/1] Loss: 1.3771\n",
      "Epoch [48/80], Iter [80/1] Loss: 1.4767\n",
      "Accuracy (train), Top1: 80.87%, Top5: 91.55%\n",
      "Accuracy (val), Top1: 84.31%, Top5: 91.99%\n",
      "Accuracy (test), Top1: 73.33%, Top5: 90.27%\n",
      "Saved best_agg model\n",
      "Epoch [49/80], Iter [10/1] Loss: 1.5842\n",
      "Epoch [49/80], Iter [20/1] Loss: 1.5414\n",
      "Epoch [49/80], Iter [30/1] Loss: 1.3459\n",
      "Epoch [49/80], Iter [40/1] Loss: 1.5675\n",
      "Epoch [49/80], Iter [50/1] Loss: 1.5620\n",
      "Epoch [49/80], Iter [60/1] Loss: 1.3663\n",
      "Epoch [49/80], Iter [70/1] Loss: 1.3876\n",
      "Epoch [49/80], Iter [80/1] Loss: 1.3970\n",
      "Accuracy (train), Top1: 80.37%, Top5: 90.70%\n",
      "Accuracy (val), Top1: 81.80%, Top5: 91.15%\n",
      "Accuracy (test), Top1: 73.59%, Top5: 90.66%\n",
      "Epoch [50/80], Iter [10/1] Loss: 1.6357\n",
      "Epoch [50/80], Iter [20/1] Loss: 1.1398\n",
      "Epoch [50/80], Iter [30/1] Loss: 1.3888\n",
      "Epoch [50/80], Iter [40/1] Loss: 1.7769\n",
      "Epoch [50/80], Iter [50/1] Loss: 1.6964\n",
      "Epoch [50/80], Iter [60/1] Loss: 1.4280\n",
      "Epoch [50/80], Iter [70/1] Loss: 1.3394\n",
      "Epoch [50/80], Iter [80/1] Loss: 1.5490\n",
      "Accuracy (train), Top1: 80.80%, Top5: 91.14%\n",
      "Accuracy (val), Top1: 82.47%, Top5: 91.65%\n",
      "Accuracy (test), Top1: 73.21%, Top5: 90.47%\n",
      "Epoch [51/80], Iter [10/1] Loss: 1.3798\n",
      "Epoch [51/80], Iter [20/1] Loss: 1.6491\n",
      "Epoch [51/80], Iter [30/1] Loss: 1.1987\n",
      "Epoch [51/80], Iter [40/1] Loss: 1.2807\n",
      "Epoch [51/80], Iter [50/1] Loss: 1.2466\n",
      "Epoch [51/80], Iter [60/1] Loss: 1.6256\n",
      "Epoch [51/80], Iter [70/1] Loss: 1.6881\n",
      "Epoch [51/80], Iter [80/1] Loss: 1.3567\n",
      "Accuracy (train), Top1: 80.95%, Top5: 91.03%\n",
      "Accuracy (val), Top1: 80.47%, Top5: 89.82%\n",
      "Accuracy (test), Top1: 74.56%, Top5: 91.39%\n",
      "Epoch [52/80], Iter [10/1] Loss: 1.2938\n",
      "Epoch [52/80], Iter [20/1] Loss: 1.5848\n",
      "Epoch [52/80], Iter [30/1] Loss: 1.2485\n",
      "Epoch [52/80], Iter [40/1] Loss: 1.4176\n",
      "Epoch [52/80], Iter [50/1] Loss: 1.2456\n",
      "Epoch [52/80], Iter [60/1] Loss: 1.4948\n",
      "Epoch [52/80], Iter [70/1] Loss: 1.2331\n",
      "Epoch [52/80], Iter [80/1] Loss: 1.3139\n",
      "Accuracy (train), Top1: 81.98%, Top5: 91.42%\n",
      "Accuracy (val), Top1: 81.14%, Top5: 91.15%\n",
      "Accuracy (test), Top1: 73.94%, Top5: 90.82%\n",
      "Epoch [53/80], Iter [10/1] Loss: 1.3776\n",
      "Epoch [53/80], Iter [20/1] Loss: 1.5568\n",
      "Epoch [53/80], Iter [30/1] Loss: 1.3962\n",
      "Epoch [53/80], Iter [40/1] Loss: 1.3316\n",
      "Epoch [53/80], Iter [50/1] Loss: 1.3503\n",
      "Epoch [53/80], Iter [60/1] Loss: 1.2779\n",
      "Epoch [53/80], Iter [70/1] Loss: 1.3491\n",
      "Epoch [53/80], Iter [80/1] Loss: 1.2376\n",
      "Accuracy (train), Top1: 82.84%, Top5: 92.01%\n",
      "Accuracy (val), Top1: 83.47%, Top5: 91.99%\n",
      "Accuracy (test), Top1: 74.32%, Top5: 90.87%\n",
      "Saved best_agg model\n",
      "Epoch [54/80], Iter [10/1] Loss: 1.3345\n",
      "Epoch [54/80], Iter [20/1] Loss: 1.4191\n",
      "Epoch [54/80], Iter [30/1] Loss: 1.3906\n",
      "Epoch [54/80], Iter [40/1] Loss: 1.4673\n",
      "Epoch [54/80], Iter [50/1] Loss: 1.3480\n",
      "Epoch [54/80], Iter [60/1] Loss: 1.2588\n",
      "Epoch [54/80], Iter [70/1] Loss: 1.2123\n",
      "Epoch [54/80], Iter [80/1] Loss: 1.2589\n",
      "Accuracy (train), Top1: 82.91%, Top5: 91.55%\n",
      "Accuracy (val), Top1: 84.97%, Top5: 92.49%\n",
      "Accuracy (test), Top1: 74.65%, Top5: 90.75%\n",
      "Saved best_agg model\n",
      "Epoch [55/80], Iter [10/1] Loss: 1.1921\n",
      "Epoch [55/80], Iter [20/1] Loss: 1.1672\n",
      "Epoch [55/80], Iter [30/1] Loss: 1.2885\n",
      "Epoch [55/80], Iter [40/1] Loss: 1.2581\n",
      "Epoch [55/80], Iter [50/1] Loss: 1.1574\n",
      "Epoch [55/80], Iter [60/1] Loss: 1.1284\n",
      "Epoch [55/80], Iter [70/1] Loss: 1.1803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [55/80], Iter [80/1] Loss: 1.2863\n",
      "Accuracy (train), Top1: 83.52%, Top5: 92.66%\n",
      "Accuracy (val), Top1: 82.47%, Top5: 90.82%\n",
      "Accuracy (test), Top1: 74.59%, Top5: 91.13%\n",
      "Epoch [56/80], Iter [10/1] Loss: 1.1583\n",
      "Epoch [56/80], Iter [20/1] Loss: 1.3995\n",
      "Epoch [56/80], Iter [30/1] Loss: 1.2225\n",
      "Epoch [56/80], Iter [40/1] Loss: 1.4418\n",
      "Epoch [56/80], Iter [50/1] Loss: 1.2584\n",
      "Epoch [56/80], Iter [60/1] Loss: 1.4052\n",
      "Epoch [56/80], Iter [70/1] Loss: 1.0740\n",
      "Epoch [56/80], Iter [80/1] Loss: 1.2673\n",
      "Accuracy (train), Top1: 83.60%, Top5: 92.33%\n",
      "Accuracy (val), Top1: 84.14%, Top5: 91.82%\n",
      "Accuracy (test), Top1: 75.25%, Top5: 91.73%\n",
      "Saved best_agg model\n",
      "Epoch [57/80], Iter [10/1] Loss: 1.1067\n",
      "Epoch [57/80], Iter [20/1] Loss: 1.5318\n",
      "Epoch [57/80], Iter [30/1] Loss: 1.1278\n",
      "Epoch [57/80], Iter [40/1] Loss: 1.2221\n",
      "Epoch [57/80], Iter [50/1] Loss: 1.0040\n",
      "Epoch [57/80], Iter [60/1] Loss: 1.1470\n",
      "Epoch [57/80], Iter [70/1] Loss: 1.2387\n",
      "Epoch [57/80], Iter [80/1] Loss: 1.3649\n",
      "Accuracy (train), Top1: 83.30%, Top5: 92.51%\n",
      "Accuracy (val), Top1: 83.14%, Top5: 92.65%\n",
      "Accuracy (test), Top1: 74.82%, Top5: 91.01%\n",
      "Epoch [58/80], Iter [10/1] Loss: 1.3329\n",
      "Epoch [58/80], Iter [20/1] Loss: 1.3988\n",
      "Epoch [58/80], Iter [30/1] Loss: 1.1295\n",
      "Epoch [58/80], Iter [40/1] Loss: 1.3283\n",
      "Epoch [58/80], Iter [50/1] Loss: 1.3847\n",
      "Epoch [58/80], Iter [60/1] Loss: 1.0802\n",
      "Epoch [58/80], Iter [70/1] Loss: 1.3520\n",
      "Epoch [58/80], Iter [80/1] Loss: 1.0567\n",
      "Accuracy (train), Top1: 83.84%, Top5: 92.25%\n",
      "Accuracy (val), Top1: 84.97%, Top5: 91.32%\n",
      "Accuracy (test), Top1: 74.78%, Top5: 91.61%\n",
      "Saved best_agg model\n",
      "Epoch [59/80], Iter [10/1] Loss: 1.1383\n",
      "Epoch [59/80], Iter [20/1] Loss: 1.1102\n",
      "Epoch [59/80], Iter [30/1] Loss: 1.2492\n",
      "Epoch [59/80], Iter [40/1] Loss: 1.4595\n",
      "Epoch [59/80], Iter [50/1] Loss: 1.1926\n",
      "Epoch [59/80], Iter [60/1] Loss: 1.1649\n",
      "Epoch [59/80], Iter [70/1] Loss: 1.2532\n",
      "Epoch [59/80], Iter [80/1] Loss: 1.3108\n",
      "Accuracy (train), Top1: 83.91%, Top5: 92.62%\n",
      "Accuracy (val), Top1: 83.14%, Top5: 92.99%\n",
      "Accuracy (test), Top1: 75.51%, Top5: 91.30%\n",
      "Epoch [60/80], Iter [10/1] Loss: 1.0333\n",
      "Epoch [60/80], Iter [20/1] Loss: 1.4425\n",
      "Epoch [60/80], Iter [30/1] Loss: 1.1875\n",
      "Epoch [60/80], Iter [40/1] Loss: 1.2477\n",
      "Epoch [60/80], Iter [50/1] Loss: 1.3133\n",
      "Epoch [60/80], Iter [60/1] Loss: 1.2082\n",
      "Epoch [60/80], Iter [70/1] Loss: 1.3226\n",
      "Epoch [60/80], Iter [80/1] Loss: 1.2903\n",
      "Accuracy (train), Top1: 84.56%, Top5: 92.38%\n",
      "Accuracy (val), Top1: 83.97%, Top5: 91.82%\n",
      "Accuracy (test), Top1: 74.75%, Top5: 91.30%\n",
      "Epoch [61/80], Iter [10/1] Loss: 1.2046\n",
      "Epoch [61/80], Iter [20/1] Loss: 1.3303\n",
      "Epoch [61/80], Iter [30/1] Loss: 1.1124\n",
      "Epoch [61/80], Iter [40/1] Loss: 1.2518\n",
      "Epoch [61/80], Iter [50/1] Loss: 1.2292\n",
      "Epoch [61/80], Iter [60/1] Loss: 1.5931\n",
      "Epoch [61/80], Iter [70/1] Loss: 1.1800\n",
      "Epoch [61/80], Iter [80/1] Loss: 1.1893\n",
      "Accuracy (train), Top1: 84.45%, Top5: 92.33%\n",
      "Accuracy (val), Top1: 86.64%, Top5: 92.49%\n",
      "Accuracy (test), Top1: 75.27%, Top5: 91.37%\n",
      "Saved best_agg model\n",
      "Epoch [62/80], Iter [10/1] Loss: 1.5460\n",
      "Epoch [62/80], Iter [20/1] Loss: 1.4635\n",
      "Epoch [62/80], Iter [30/1] Loss: 1.1521\n",
      "Epoch [62/80], Iter [40/1] Loss: 1.2031\n",
      "Epoch [62/80], Iter [50/1] Loss: 1.2403\n",
      "Epoch [62/80], Iter [60/1] Loss: 1.2383\n",
      "Epoch [62/80], Iter [70/1] Loss: 1.1739\n",
      "Epoch [62/80], Iter [80/1] Loss: 1.0688\n",
      "Accuracy (train), Top1: 83.71%, Top5: 92.23%\n",
      "Accuracy (val), Top1: 83.47%, Top5: 91.82%\n",
      "Accuracy (test), Top1: 75.16%, Top5: 91.28%\n",
      "Epoch [63/80], Iter [10/1] Loss: 1.3994\n",
      "Epoch [63/80], Iter [20/1] Loss: 1.6449\n",
      "Epoch [63/80], Iter [30/1] Loss: 0.9576\n",
      "Epoch [63/80], Iter [40/1] Loss: 0.9082\n",
      "Epoch [63/80], Iter [50/1] Loss: 1.0880\n",
      "Epoch [63/80], Iter [60/1] Loss: 1.0817\n",
      "Epoch [63/80], Iter [70/1] Loss: 1.4182\n",
      "Epoch [63/80], Iter [80/1] Loss: 1.2883\n",
      "Accuracy (train), Top1: 83.02%, Top5: 92.36%\n",
      "Accuracy (val), Top1: 83.97%, Top5: 92.99%\n",
      "Accuracy (test), Top1: 75.27%, Top5: 91.80%\n",
      "Epoch [64/80], Iter [10/1] Loss: 1.1368\n",
      "Epoch [64/80], Iter [20/1] Loss: 1.2423\n",
      "Epoch [64/80], Iter [30/1] Loss: 1.1441\n",
      "Epoch [64/80], Iter [40/1] Loss: 1.0197\n",
      "Epoch [64/80], Iter [50/1] Loss: 1.2244\n",
      "Epoch [64/80], Iter [60/1] Loss: 1.5757\n",
      "Epoch [64/80], Iter [70/1] Loss: 1.0484\n",
      "Epoch [64/80], Iter [80/1] Loss: 1.1277\n",
      "Accuracy (train), Top1: 83.73%, Top5: 92.14%\n",
      "Accuracy (val), Top1: 85.14%, Top5: 92.65%\n",
      "Accuracy (test), Top1: 75.73%, Top5: 91.40%\n",
      "Epoch [65/80], Iter [10/1] Loss: 1.7818\n",
      "Epoch [65/80], Iter [20/1] Loss: 1.1695\n",
      "Epoch [65/80], Iter [30/1] Loss: 1.3412\n",
      "Epoch [65/80], Iter [40/1] Loss: 1.2562\n",
      "Epoch [65/80], Iter [50/1] Loss: 1.2004\n",
      "Epoch [65/80], Iter [60/1] Loss: 1.5957\n",
      "Epoch [65/80], Iter [70/1] Loss: 1.1791\n",
      "Epoch [65/80], Iter [80/1] Loss: 1.2229\n",
      "Accuracy (train), Top1: 83.24%, Top5: 92.77%\n",
      "Accuracy (val), Top1: 85.31%, Top5: 90.65%\n",
      "Accuracy (test), Top1: 75.61%, Top5: 91.30%\n",
      "Epoch [66/80], Iter [10/1] Loss: 1.1846\n",
      "Epoch [66/80], Iter [20/1] Loss: 1.3660\n",
      "Epoch [66/80], Iter [30/1] Loss: 1.5395\n",
      "Epoch [66/80], Iter [40/1] Loss: 1.1629\n",
      "Epoch [66/80], Iter [50/1] Loss: 1.3930\n",
      "Epoch [66/80], Iter [60/1] Loss: 1.0905\n",
      "Epoch [66/80], Iter [70/1] Loss: 1.0366\n",
      "Epoch [66/80], Iter [80/1] Loss: 1.3200\n",
      "Accuracy (train), Top1: 83.28%, Top5: 92.34%\n",
      "Accuracy (val), Top1: 84.31%, Top5: 91.82%\n",
      "Accuracy (test), Top1: 75.13%, Top5: 91.23%\n",
      "Epoch [67/80], Iter [10/1] Loss: 1.3939\n",
      "Epoch [67/80], Iter [20/1] Loss: 1.3593\n",
      "Epoch [67/80], Iter [30/1] Loss: 1.4607\n",
      "Epoch [67/80], Iter [40/1] Loss: 1.2623\n",
      "Epoch [67/80], Iter [50/1] Loss: 1.2453\n",
      "Epoch [67/80], Iter [60/1] Loss: 1.3148\n",
      "Epoch [67/80], Iter [70/1] Loss: 1.3232\n",
      "Epoch [67/80], Iter [80/1] Loss: 1.1907\n",
      "Accuracy (train), Top1: 83.76%, Top5: 92.12%\n",
      "Accuracy (val), Top1: 84.31%, Top5: 92.15%\n",
      "Accuracy (test), Top1: 73.97%, Top5: 90.85%\n",
      "Epoch [68/80], Iter [10/1] Loss: 1.2345\n",
      "Epoch [68/80], Iter [20/1] Loss: 1.3124\n",
      "Epoch [68/80], Iter [30/1] Loss: 1.2395\n",
      "Epoch [68/80], Iter [40/1] Loss: 1.2759\n",
      "Epoch [68/80], Iter [50/1] Loss: 1.0243\n",
      "Epoch [68/80], Iter [60/1] Loss: 1.3160\n",
      "Epoch [68/80], Iter [70/1] Loss: 1.2144\n",
      "Epoch [68/80], Iter [80/1] Loss: 1.2533\n",
      "Accuracy (train), Top1: 84.19%, Top5: 93.01%\n",
      "Accuracy (val), Top1: 84.81%, Top5: 93.16%\n",
      "Accuracy (test), Top1: 74.80%, Top5: 91.16%\n",
      "Epoch [69/80], Iter [10/1] Loss: 1.4216\n",
      "Epoch [69/80], Iter [20/1] Loss: 1.1402\n",
      "Epoch [69/80], Iter [30/1] Loss: 1.1090\n",
      "Epoch [69/80], Iter [40/1] Loss: 1.2806\n",
      "Epoch [69/80], Iter [50/1] Loss: 1.4335\n",
      "Epoch [69/80], Iter [60/1] Loss: 1.3622\n",
      "Epoch [69/80], Iter [70/1] Loss: 1.1900\n",
      "Epoch [69/80], Iter [80/1] Loss: 1.2635\n",
      "Accuracy (train), Top1: 84.30%, Top5: 92.29%\n",
      "Accuracy (val), Top1: 84.31%, Top5: 92.32%\n",
      "Accuracy (test), Top1: 75.84%, Top5: 91.44%\n",
      "Epoch [70/80], Iter [10/1] Loss: 1.3190\n",
      "Epoch [70/80], Iter [20/1] Loss: 1.4224\n",
      "Epoch [70/80], Iter [30/1] Loss: 1.1508\n",
      "Epoch [70/80], Iter [40/1] Loss: 1.3358\n",
      "Epoch [70/80], Iter [50/1] Loss: 1.1367\n",
      "Epoch [70/80], Iter [60/1] Loss: 1.1278\n",
      "Epoch [70/80], Iter [70/1] Loss: 1.1495\n",
      "Epoch [70/80], Iter [80/1] Loss: 1.1021\n",
      "Accuracy (train), Top1: 85.39%, Top5: 92.81%\n",
      "Accuracy (val), Top1: 83.64%, Top5: 90.15%\n",
      "Accuracy (test), Top1: 75.70%, Top5: 91.66%\n",
      "Epoch [71/80], Iter [10/1] Loss: 1.4535\n",
      "Epoch [71/80], Iter [20/1] Loss: 1.3986\n",
      "Epoch [71/80], Iter [30/1] Loss: 0.9090\n",
      "Epoch [71/80], Iter [40/1] Loss: 1.2730\n",
      "Epoch [71/80], Iter [50/1] Loss: 1.1946\n",
      "Epoch [71/80], Iter [60/1] Loss: 1.2640\n",
      "Epoch [71/80], Iter [70/1] Loss: 1.2115\n",
      "Epoch [71/80], Iter [80/1] Loss: 1.0818\n",
      "Accuracy (train), Top1: 83.82%, Top5: 92.33%\n",
      "Accuracy (val), Top1: 83.81%, Top5: 90.98%\n",
      "Accuracy (test), Top1: 75.39%, Top5: 91.20%\n",
      "Epoch [72/80], Iter [10/1] Loss: 1.2666\n",
      "Epoch [72/80], Iter [20/1] Loss: 1.5214\n",
      "Epoch [72/80], Iter [30/1] Loss: 1.0582\n",
      "Epoch [72/80], Iter [40/1] Loss: 1.3484\n",
      "Epoch [72/80], Iter [50/1] Loss: 1.1276\n",
      "Epoch [72/80], Iter [60/1] Loss: 1.3519\n",
      "Epoch [72/80], Iter [70/1] Loss: 1.7123\n",
      "Epoch [72/80], Iter [80/1] Loss: 1.1206\n",
      "Accuracy (train), Top1: 83.19%, Top5: 91.55%\n",
      "Accuracy (val), Top1: 84.97%, Top5: 93.32%\n",
      "Accuracy (test), Top1: 75.60%, Top5: 91.61%\n",
      "Epoch [73/80], Iter [10/1] Loss: 1.3758\n",
      "Epoch [73/80], Iter [20/1] Loss: 1.3162\n",
      "Epoch [73/80], Iter [30/1] Loss: 1.0696\n",
      "Epoch [73/80], Iter [40/1] Loss: 1.3715\n",
      "Epoch [73/80], Iter [50/1] Loss: 1.3423\n",
      "Epoch [73/80], Iter [60/1] Loss: 1.2616\n",
      "Epoch [73/80], Iter [70/1] Loss: 1.3563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [73/80], Iter [80/1] Loss: 1.0593\n",
      "Accuracy (train), Top1: 84.39%, Top5: 92.62%\n",
      "Accuracy (val), Top1: 85.14%, Top5: 91.99%\n",
      "Accuracy (test), Top1: 75.70%, Top5: 91.80%\n",
      "Epoch [74/80], Iter [10/1] Loss: 1.1572\n",
      "Epoch [74/80], Iter [20/1] Loss: 1.2218\n",
      "Epoch [74/80], Iter [30/1] Loss: 1.3017\n",
      "Epoch [74/80], Iter [40/1] Loss: 1.1214\n",
      "Epoch [74/80], Iter [50/1] Loss: 1.1956\n",
      "Epoch [74/80], Iter [60/1] Loss: 1.2700\n",
      "Epoch [74/80], Iter [70/1] Loss: 1.4377\n",
      "Epoch [74/80], Iter [80/1] Loss: 1.4107\n",
      "Accuracy (train), Top1: 83.34%, Top5: 91.99%\n",
      "Accuracy (val), Top1: 85.98%, Top5: 92.15%\n",
      "Accuracy (test), Top1: 75.51%, Top5: 91.63%\n",
      "Epoch [75/80], Iter [10/1] Loss: 1.1759\n",
      "Epoch [75/80], Iter [20/1] Loss: 1.1318\n",
      "Epoch [75/80], Iter [30/1] Loss: 1.1163\n",
      "Epoch [75/80], Iter [40/1] Loss: 1.3596\n",
      "Epoch [75/80], Iter [50/1] Loss: 1.1927\n",
      "Epoch [75/80], Iter [60/1] Loss: 1.0970\n",
      "Epoch [75/80], Iter [70/1] Loss: 1.0275\n",
      "Epoch [75/80], Iter [80/1] Loss: 1.2595\n",
      "Accuracy (train), Top1: 84.17%, Top5: 92.47%\n",
      "Accuracy (val), Top1: 84.64%, Top5: 93.16%\n",
      "Accuracy (test), Top1: 75.11%, Top5: 91.34%\n",
      "Epoch [76/80], Iter [10/1] Loss: 1.0303\n",
      "Epoch [76/80], Iter [20/1] Loss: 1.1274\n",
      "Epoch [76/80], Iter [30/1] Loss: 1.0638\n",
      "Epoch [76/80], Iter [40/1] Loss: 1.0386\n",
      "Epoch [76/80], Iter [50/1] Loss: 1.1943\n",
      "Epoch [76/80], Iter [60/1] Loss: 1.0321\n",
      "Epoch [76/80], Iter [70/1] Loss: 1.2269\n",
      "Epoch [76/80], Iter [80/1] Loss: 1.0914\n",
      "Accuracy (train), Top1: 84.82%, Top5: 92.73%\n",
      "Accuracy (val), Top1: 85.98%, Top5: 92.82%\n",
      "Accuracy (test), Top1: 75.13%, Top5: 91.53%\n",
      "Epoch [77/80], Iter [10/1] Loss: 1.4515\n",
      "Epoch [77/80], Iter [20/1] Loss: 1.2759\n",
      "Epoch [77/80], Iter [30/1] Loss: 1.2624\n",
      "Epoch [77/80], Iter [40/1] Loss: 1.1488\n",
      "Epoch [77/80], Iter [50/1] Loss: 0.8807\n",
      "Epoch [77/80], Iter [60/1] Loss: 1.1979\n",
      "Epoch [77/80], Iter [70/1] Loss: 1.1528\n",
      "Epoch [77/80], Iter [80/1] Loss: 1.0109\n",
      "Accuracy (train), Top1: 84.62%, Top5: 92.34%\n",
      "Accuracy (val), Top1: 85.31%, Top5: 91.49%\n",
      "Accuracy (test), Top1: 75.60%, Top5: 91.53%\n",
      "Epoch [78/80], Iter [10/1] Loss: 1.4533\n",
      "Epoch [78/80], Iter [20/1] Loss: 1.0748\n",
      "Epoch [78/80], Iter [30/1] Loss: 1.1438\n",
      "Epoch [78/80], Iter [40/1] Loss: 1.3078\n",
      "Epoch [78/80], Iter [50/1] Loss: 1.3056\n",
      "Epoch [78/80], Iter [60/1] Loss: 1.0260\n",
      "Epoch [78/80], Iter [70/1] Loss: 1.5357\n",
      "Epoch [78/80], Iter [80/1] Loss: 1.1537\n",
      "Accuracy (train), Top1: 84.13%, Top5: 92.29%\n",
      "Accuracy (val), Top1: 85.98%, Top5: 92.82%\n",
      "Accuracy (test), Top1: 75.61%, Top5: 91.39%\n",
      "Saved best_agg model\n",
      "Epoch [79/80], Iter [10/1] Loss: 1.2195\n",
      "Epoch [79/80], Iter [20/1] Loss: 1.2252\n",
      "Epoch [79/80], Iter [30/1] Loss: 0.9549\n",
      "Epoch [79/80], Iter [40/1] Loss: 1.1787\n",
      "Epoch [79/80], Iter [50/1] Loss: 1.3089\n",
      "Epoch [79/80], Iter [60/1] Loss: 1.2345\n",
      "Epoch [79/80], Iter [70/1] Loss: 1.4632\n",
      "Epoch [79/80], Iter [80/1] Loss: 1.2318\n",
      "Accuracy (train), Top1: 84.75%, Top5: 92.36%\n",
      "Accuracy (val), Top1: 84.81%, Top5: 93.66%\n",
      "Accuracy (test), Top1: 75.79%, Top5: 91.80%\n",
      "Epoch [80/80], Iter [10/1] Loss: 1.5005\n",
      "Epoch [80/80], Iter [20/1] Loss: 1.3743\n",
      "Epoch [80/80], Iter [30/1] Loss: 1.2714\n",
      "Epoch [80/80], Iter [40/1] Loss: 1.0992\n",
      "Epoch [80/80], Iter [50/1] Loss: 1.1160\n",
      "Epoch [80/80], Iter [60/1] Loss: 1.2364\n",
      "Epoch [80/80], Iter [70/1] Loss: 1.1729\n",
      "Epoch [80/80], Iter [80/1] Loss: 1.3241\n",
      "Accuracy (train), Top1: 84.49%, Top5: 92.23%\n",
      "Accuracy (val), Top1: 81.80%, Top5: 91.32%\n",
      "Accuracy (test), Top1: 76.16%, Top5: 91.44%\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model, dataloaders, 80, device, log_freq, data_path + '/../log/')\n",
    "torch.cuda.empty_cache()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7060528682483421, 0.7371284784851494, 0.7459745315405039, 0.777114755504734, 0.7852361549387922, 0.8043332899009789, 0.8325808146497705, 0.82355457675326, 0.8366909452847072, 0.835416506934952, 0.8422107329735389, 0.8273080587387085, 0.8427379216466632, 0.8393583022631131, 0.8507679334053626, 0.8336113593080542, 0.8340685557533096, 0.8498040424598442, 0.8479510100333245, 0.8475243743959364, 0.8464256249941312, 0.8418444133066869, 0.8514138899006687, 0.8510753040785318, 0.862388361286331, 0.8423665093851613, 0.8257665516255976, 0.8595915168196291, 0.8568565308392703, 0.8464279240304298, 0.8328731053478116, 0.8296145639576755, 0.8524267247744969, 0.8512327356652899, 0.8537118480755732, 0.8546351698728708, 0.8441077241530786, 0.8429082284916888, 0.8471080988318056, 0.8455572331344688, 0.8447452036889045, 0.8426420636229462, 0.846158097078512, 0.8485401895020034, 0.8364431674663837, 0.838502151625497, 0.8474256730341649, 0.8322586790545956, 0.8511564286200555, 0.8530022410246042, 0.8225560803989788, 0.8143722722818564, 0.8254533803069984, 0.8407106622234806, 0.8297969482757233, 0.8252774044707581, 0.8171992557389396, 0.814945359151442, 0.8129664685699965, 0.8248212121345184, 0.8354304288769816, 0.8360685876437596, 0.8291887352754782, 0.8408235763455485, 0.8354537984827063, 0.8186729478312063, 0.8179762893980675, 0.8281182771200662, 0.7984548767844399, 0.8262934763352949, 0.8321901448480375, 0.8214771682089501, 0.8335511769567218, 0.8361830207017752, 0.8316508518470512, 0.8185200848422207, 0.8443966841959691, 0.8227860554234012, 0.830135695881896, 0.836860057416853]\n",
      "\n",
      "[0.05582736799007525, 0.06277163089860392, 0.06818409563730592, 0.06612427799921969, 0.05794978554805984, 0.05575442536855522, 0.05449806704469349, 0.051608661473121334, 0.051362071429257805, 0.051401264601103634, 0.05210020872966751, 0.05009809237621401, 0.050418713899410286, 0.04873721921087607, 0.050215127472968205, 0.0482630911283195, 0.047468341320105224, 0.04882268202693566, 0.04760279546937217, 0.04955646478692475, 0.04729687128944889, 0.048850443720331656, 0.04949604201575984, 0.04926360503810903, 0.05104036094701808, 0.04821301079557642, 0.04733552004251143, 0.04956299686075553, 0.04889671354676071, 0.048461234379235815, 0.04818851529094188, 0.04879328729989736, 0.048130269648264286, 0.04811992723008861, 0.05027990518947658, 0.05011333403703959, 0.04842639580854903, 0.049092135027698845, 0.04921679087388127, 0.049651725906068866, 0.04953904563318128, 0.049786724555103676, 0.04976440625994102, 0.04903497832381855, 0.04964682678489581, 0.04918467427563408, 0.049336003362322634, 0.04914548114428054, 0.05146767512854675, 0.050576575606575476, 0.04911554209968966, 0.04739049935470457, 0.04932511650511752, 0.05024996584119356, 0.049738821958232184, 0.04953469107251452, 0.04989940503045269, 0.04884227851162786, 0.04903824453282615, 0.04833930003983171, 0.05017484562552493, 0.05049818914140696, 0.05008883810723606, 0.05065006248490966, 0.050205873487436256, 0.04949332032676624, 0.047712754073512297, 0.04980686560030217, 0.04729088333070926, 0.04994567459368188, 0.05046443940828676, 0.0498378934217212, 0.05000881891211738, 0.050302767518745815, 0.0508215324750737, 0.048798186522301126, 0.05015198270911756, 0.04980087770230096, 0.05023526833595141, 0.05019117632637853]\n",
      "\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(trainer.mean_attr_accs)\n",
    "print()\n",
    "print(trainer.mean_drop_ratio)\n",
    "print()\n",
    "print(trainer.mean_sigmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".  ..  urdtc\r\n"
     ]
    }
   ],
   "source": [
    "!ls -a ../"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
